{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 7284, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"C:\\Python311\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6098, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"C:\\Python311\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 7379, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"C:\\Python311\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Failed to generate JSON. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'json_validate_failed', 'failed_generation': '{\\n   \"title\": \"University and R&D Teams\",\\n   \"summary\": \"The community revolves around the University, which has relationships with R&D Teams. The University is an institution of higher education, and R&D Teams operate at the university.\",\\n   \"rating\": 2.0,\\n   \"rating_explanation\": \"The impact severity rating is low due to the lack of potential for unrest or conflict within the community.\",\\n   \"findings\": [\\n      {\\n         \"summary\": \"University as an institution of higher education\",\\n         \"explanation\": \"The University is an institution of higher education, which is a significant entity in this community. This institution is the common link between all other entities, suggesting its significance in the community. [Data: Entities (104), Relationships (115, 126)]\"\\n      },\\n      {\\n         \"summary\": \"R&D Teams operating at the university,\\\\n         \"explanation\": \"R&D Teams operate at the university, which is a key aspect of this community. The nature of R&D Teams and their operations could be a potential source of innovation, depending on their objectives and the reactions they provoke. [Data: Entities (103), Relationships (126)]\"\\\\n      }\\\\n   ]\\\\n}'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"C:\\Python311\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6628, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"C:\\Python311\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 7536, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"C:\\Python311\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Failed to generate JSON. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'json_validate_failed', 'failed_generation': '{\\n   \"title\": \"Secondary School Students and Supporting Institutions\",\\n   \"summary\": \"The community revolves around secondary school students who participate in university classes and activities. The students are supported by Gdańsk University of Technology, institutions taking care of exceptionally talented young people, the principal of the student\\'s school, and parents or legal guardians.\",\\n   \"rating\": 4.0,\\n   \"rating_explanation\": \"The impact severity rating is moderate due to the potential for academic and personal growth, as well as potential challenges in balancing secondary school and university responsibilities.\",\\n   \"findings\": [\\n      {\\n         \"summary\": \"Secondary school students as exceptionally talented individuals\",\\n         \"explanation\": \"Secondary school students are exceptionally talented individuals who not only attend university classes but also participate in university activities. This unique group of students demonstrates a high level of academic aptitude, allowing them to engage in university-level coursework while still in secondary school. Their participation in university activities further highlights their enthusiasm for learning and their ability to thrive in a collegiate environment. [Data: Entities (80), Relationships (109, 110, 111, 112, +more)]\"\\n      },\\n      {\\n         \"summary\": \"Gdańsk University of Technology\\'s role in the community\",\\n         \"explanation\": \"Gdańsk University of Technology has established a collaborative program with secondary school students. As part of this initiative, secondary school students participate in various activities organized by the university. Furthermore, these students are also given the opportunity to attend classes at Gdańsk University of Technology, allowing them to gain valuable academic experience. [Data: Relationships (109)]\"\\n      },\\n      {\\n         \"summary\": \"Institutions taking care of exceptionally talented young people\",\\n         \"explanation\": \"Institutions taking care of exceptionally talented young people support secondary school students. These institutions play a crucial role in providing resources and guidance to these students, enabling them to excel in their academic pursuits. [Data: Entities (81), Relationships (110)]\"\\n      },\\n      {\\n         \"summary\": \"Role of the principal and parents or legal guardians\",\\n         \"explanation\": \"The principal of the student\\'s school and parents or legal guardians give consent for secondary school students to attend university classes. Their approval is essential in ensuring that the students are able to participate in university activities. [Data: Entities (82, 83), Relationships (111, 112)]\\\\n}'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Failed to generate JSON. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'json_validate_failed', 'failed_generation': '{\\n   \"title\": \"Gdańsk University of Technology Faculty Community\",\\n   \"summary\": \"The community revolves around the Faculty within Gdańsk University of Technology, which has its own autonomy and independence within the larger institution. The Faculty has relationships with various entities such as Year Presidents/Starostas, Examiner, Specialist, and Representative of the Student Government, all of which are associated with the academic experience of students.\",\\n   \"rating\": 4.5,\\n   \"rating_explanation\": \"The impact severity rating is moderate due to the potential for academic unrest or conflict within the university, depending on the nature of the faculty and student government interactions.\",\\n   \"findings\": [\\n      {\\n         \"Faculty as the central entity\",\\n         \"explanation\": \"The Faculty is the central entity in this community, having its own website, student council, and autonomy within the university. This suggests a level of independence and authority within the larger institution. The Faculty\\'s association with various entities could potentially lead to issues such as academic unrest or conflict, depending on the nature of the interactions. [Data: Entities (29), Relationships (54, 55, 56, 57, 31, 32, 35, 36, +more)]\"\\n      },\\n      {\\n         \"summary\": \"Year Presidents/Starostas\\' role in the community\",\\n         \"explanation\": \"Year Presidents/Starostas are student representatives in the Faculty, playing a crucial role in shaping the academic experience of students. The relationship between Year Presidents/Starostas and the Faculty is significant in understanding the dynamics of this community. [Data: Entities (53), Relationships (54)]\"\\n      },\\n      {\\n         \"summary\": \"Examiner\\'s role in the community\",\\n         \"explanation\": \"Examiner is responsible for conducting exams and assessing students in the Faculty. This role is vital in shaping the academic experience of students. The relationship between Examiner and the Faculty is crucial in understanding the dynamics of this community. [Data: Entities (54), Relationships (55)]\"\\n      },\\n      {\\n         \"summary\": \"Student Government\\'s role in the community\",\\n         \"explanation\": \"The Student Government is an organization that consults with the Faculty board, offering written opinions on study plans and curriculums. This collaborative approach enables the Faculty board to make decisions that benefit the student community as a whole. The relationship between the Student Government and the Rector is significant in understanding the dynamics of this community. [Data: Entities (58), Relationships (73)]\"\\n      },\\n      {\\n         \"summary\": \"Academic Teacher\\'s role in the community\",\\n         \"explanation\": \"An Academic Teacher is a person responsible for teaching, guiding, and assessing students. They provide information on consultation hours and conduct classes, making decisions about exemptions. The relationship between Academic Teacher and Gdańsk University of Technology is significant in understanding the dynamics of this community. [Data: Entities (33), Relationships (94)]\"\\n      }\\n   ]\\n}'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Failed to generate JSON. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'json_validate_failed', 'failed_generation': '{\\n   \"title\": \"Gdańsk University of Technology and Associated Entities\",\\n   \"summary\": \"The community revolves around Gdańsk University of Technology, which is associated with several entities such as Moja PG, Gdańsk Tech Student Government, eNauczanie, among others. These entities are interconnected, with Gdańsk University of Technology\\'s electronic system being Moja PG, and the university consulting with Gdańsk Tech Student Government on diploma examination rules.\",\\n   \"rating\": 4.0,\\n   \"rating_explanation\": \"The impact severity rating is moderate due to the potential for academic and administrative issues within the university, depending on the effectiveness of the entities involved.\",\\n   \"findings\": [\\n      {\\n         \"summary\": \"Gdańsk University of Technology as the central entity\",\\n         \"explanation\": \"Gdańsk University of Technology is the central entity in this community, being associated with several other entities. The university\\'s electronic system is Moja PG, which provides students with essential academic information. [Data: Entities (57), Relationships (1, 90, 50, 93, 95, 96, +more)]\"\\n      },\\n      {\\n         \"summary\": \"Moja PG\\'s role in the community\",\\n         \"explanation\": \"Moja PG is Gdańsk University of Technology\\'s electronic system, providing students with essential academic information. This system is crucial in the community, as it enables students to access their academic information and track their progress. [Data: (1), Entities (1), Relationships (1)]\"\\n      },\\n      {\\n         \"summary\": \"Gdańsk Tech Student Government\\'s role in the community\",\\n         \"explanation\": \"Gdańsk Tech Student Government is a representative body for students that plays a crucial role in consulting with the faculty board on diploma examination rules. This ensures that the voices and concerns of students are heard and considered in the decision-making process. [Data: Entities (51), Relationships (90)]\"\\n      },\\n      {\\n         \"summary\": \"eNauczanie\\'s role in the community\",\\n         \"explanation\": \"eNauczanie is an online platform used by Gdańsk University of Technology students with access to course materials and information. This platform is significant in the community, as it enables students to access essential academic information. [Data: Entities (50), Relationships (50)]\"\\n      },\\n      {\\n         \"summary\": \"The role of the Chairperson of the Diploma Examination Committee\",\\n         \"explanation\": \"The Chairperson of the Diploma Examination Committee is an employee of Gdańsk University of Technology, leading the diploma examination process. This role is crucial in the community, as it ensures the smooth operation of the diploma examination process. [Data: Entities (68), Relationships (93)]\\\\n      }\\\\n   ]\\\\n}'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}

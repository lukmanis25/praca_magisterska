{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 7284, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"C:\\Python311\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6098, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"C:\\Python311\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 7379, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"C:\\Python311\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Failed to generate JSON. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'json_validate_failed', 'failed_generation': '{\\n   \"title\": \"University and R&D Teams\",\\n   \"summary\": \"The community revolves around the University, which has relationships with R&D Teams. The University is an institution of higher education, and R&D Teams operate at the university.\",\\n   \"rating\": 2.0,\\n   \"rating_explanation\": \"The impact severity rating is low due to the lack of potential for unrest or conflict within the community.\",\\n   \"findings\": [\\n      {\\n         \"summary\": \"University as an institution of higher education\",\\n         \"explanation\": \"The University is an institution of higher education, which is a significant entity in this community. This institution is the common link between all other entities, suggesting its significance in the community. [Data: Entities (104), Relationships (115, 126)]\"\\n      },\\n      {\\n         \"summary\": \"R&D Teams operating at the university,\\\\n         \"explanation\": \"R&D Teams operate at the university, which is a key aspect of this community. The nature of R&D Teams and their operations could be a potential source of innovation, depending on their objectives and the reactions they provoke. [Data: Entities (103), Relationships (126)]\"\\\\n      }\\\\n   ]\\\\n}'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"C:\\Python311\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6628, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"C:\\Python311\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\services\\rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 7536, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\index\\operations\\summarize_communities\\community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\llm\\openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\openai\\services\\openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\fnllm\\base\\base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\language_model\\providers\\fnllm\\utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\callbacks\\workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"C:\\Projekty_magister_sem3\\magisterka\\praca_magisterska\\method2_graphrag\\venv\\Lib\\site-packages\\graphrag\\callbacks\\file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"C:\\Python311\\Lib\\json\\__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 202, in encode\n    chunks = list(chunks)\n             ^^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\json\\encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Failed to generate JSON. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'json_validate_failed', 'failed_generation': '{\\n   \"title\": \"Secondary School Students and Supporting Institutions\",\\n   \"summary\": \"The community revolves around secondary school students who participate in university classes and activities. The students are supported by Gdańsk University of Technology, institutions taking care of exceptionally talented young people, the principal of the student\\'s school, and parents or legal guardians.\",\\n   \"rating\": 4.0,\\n   \"rating_explanation\": \"The impact severity rating is moderate due to the potential for academic and personal growth, as well as potential challenges in balancing secondary school and university responsibilities.\",\\n   \"findings\": [\\n      {\\n         \"summary\": \"Secondary school students as exceptionally talented individuals\",\\n         \"explanation\": \"Secondary school students are exceptionally talented individuals who not only attend university classes but also participate in university activities. This unique group of students demonstrates a high level of academic aptitude, allowing them to engage in university-level coursework while still in secondary school. Their participation in university activities further highlights their enthusiasm for learning and their ability to thrive in a collegiate environment. [Data: Entities (80), Relationships (109, 110, 111, 112, +more)]\"\\n      },\\n      {\\n         \"summary\": \"Gdańsk University of Technology\\'s role in the community\",\\n         \"explanation\": \"Gdańsk University of Technology has established a collaborative program with secondary school students. As part of this initiative, secondary school students participate in various activities organized by the university. Furthermore, these students are also given the opportunity to attend classes at Gdańsk University of Technology, allowing them to gain valuable academic experience. [Data: Relationships (109)]\"\\n      },\\n      {\\n         \"summary\": \"Institutions taking care of exceptionally talented young people\",\\n         \"explanation\": \"Institutions taking care of exceptionally talented young people support secondary school students. These institutions play a crucial role in providing resources and guidance to these students, enabling them to excel in their academic pursuits. [Data: Entities (81), Relationships (110)]\"\\n      },\\n      {\\n         \"summary\": \"Role of the principal and parents or legal guardians\",\\n         \"explanation\": \"The principal of the student\\'s school and parents or legal guardians give consent for secondary school students to attend university classes. Their approval is essential in ensuring that the students are able to participate in university activities. [Data: Entities (82, 83), Relationships (111, 112)]\\\\n}'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Failed to generate JSON. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'json_validate_failed', 'failed_generation': '{\\n   \"title\": \"Gdańsk University of Technology Faculty Community\",\\n   \"summary\": \"The community revolves around the Faculty within Gdańsk University of Technology, which has its own autonomy and independence within the larger institution. The Faculty has relationships with various entities such as Year Presidents/Starostas, Examiner, Specialist, and Representative of the Student Government, all of which are associated with the academic experience of students.\",\\n   \"rating\": 4.5,\\n   \"rating_explanation\": \"The impact severity rating is moderate due to the potential for academic unrest or conflict within the university, depending on the nature of the faculty and student government interactions.\",\\n   \"findings\": [\\n      {\\n         \"Faculty as the central entity\",\\n         \"explanation\": \"The Faculty is the central entity in this community, having its own website, student council, and autonomy within the university. This suggests a level of independence and authority within the larger institution. The Faculty\\'s association with various entities could potentially lead to issues such as academic unrest or conflict, depending on the nature of the interactions. [Data: Entities (29), Relationships (54, 55, 56, 57, 31, 32, 35, 36, +more)]\"\\n      },\\n      {\\n         \"summary\": \"Year Presidents/Starostas\\' role in the community\",\\n         \"explanation\": \"Year Presidents/Starostas are student representatives in the Faculty, playing a crucial role in shaping the academic experience of students. The relationship between Year Presidents/Starostas and the Faculty is significant in understanding the dynamics of this community. [Data: Entities (53), Relationships (54)]\"\\n      },\\n      {\\n         \"summary\": \"Examiner\\'s role in the community\",\\n         \"explanation\": \"Examiner is responsible for conducting exams and assessing students in the Faculty. This role is vital in shaping the academic experience of students. The relationship between Examiner and the Faculty is crucial in understanding the dynamics of this community. [Data: Entities (54), Relationships (55)]\"\\n      },\\n      {\\n         \"summary\": \"Student Government\\'s role in the community\",\\n         \"explanation\": \"The Student Government is an organization that consults with the Faculty board, offering written opinions on study plans and curriculums. This collaborative approach enables the Faculty board to make decisions that benefit the student community as a whole. The relationship between the Student Government and the Rector is significant in understanding the dynamics of this community. [Data: Entities (58), Relationships (73)]\"\\n      },\\n      {\\n         \"summary\": \"Academic Teacher\\'s role in the community\",\\n         \"explanation\": \"An Academic Teacher is a person responsible for teaching, guiding, and assessing students. They provide information on consultation hours and conduct classes, making decisions about exemptions. The relationship between Academic Teacher and Gdańsk University of Technology is significant in understanding the dynamics of this community. [Data: Entities (33), Relationships (94)]\"\\n      }\\n   ]\\n}'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Failed to generate JSON. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'json_validate_failed', 'failed_generation': '{\\n   \"title\": \"Gdańsk University of Technology and Associated Entities\",\\n   \"summary\": \"The community revolves around Gdańsk University of Technology, which is associated with several entities such as Moja PG, Gdańsk Tech Student Government, eNauczanie, among others. These entities are interconnected, with Gdańsk University of Technology\\'s electronic system being Moja PG, and the university consulting with Gdańsk Tech Student Government on diploma examination rules.\",\\n   \"rating\": 4.0,\\n   \"rating_explanation\": \"The impact severity rating is moderate due to the potential for academic and administrative issues within the university, depending on the effectiveness of the entities involved.\",\\n   \"findings\": [\\n      {\\n         \"summary\": \"Gdańsk University of Technology as the central entity\",\\n         \"explanation\": \"Gdańsk University of Technology is the central entity in this community, being associated with several other entities. The university\\'s electronic system is Moja PG, which provides students with essential academic information. [Data: Entities (57), Relationships (1, 90, 50, 93, 95, 96, +more)]\"\\n      },\\n      {\\n         \"summary\": \"Moja PG\\'s role in the community\",\\n         \"explanation\": \"Moja PG is Gdańsk University of Technology\\'s electronic system, providing students with essential academic information. This system is crucial in the community, as it enables students to access their academic information and track their progress. [Data: (1), Entities (1), Relationships (1)]\"\\n      },\\n      {\\n         \"summary\": \"Gdańsk Tech Student Government\\'s role in the community\",\\n         \"explanation\": \"Gdańsk Tech Student Government is a representative body for students that plays a crucial role in consulting with the faculty board on diploma examination rules. This ensures that the voices and concerns of students are heard and considered in the decision-making process. [Data: Entities (51), Relationships (90)]\"\\n      },\\n      {\\n         \"summary\": \"eNauczanie\\'s role in the community\",\\n         \"explanation\": \"eNauczanie is an online platform used by Gdańsk University of Technology students with access to course materials and information. This platform is significant in the community, as it enables students to access essential academic information. [Data: Entities (50), Relationships (50)]\"\\n      },\\n      {\\n         \"summary\": \"The role of the Chairperson of the Diploma Examination Committee\",\\n         \"explanation\": \"The Chairperson of the Diploma Examination Committee is an employee of Gdańsk University of Technology, leading the diploma examination process. This role is crucial in the community, as it ensures the smooth operation of the diploma examination process. [Data: Entities (68), Relationships (93)]\\\\n      }\\\\n   ]\\\\n}'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Error Invoking LLM",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 78, in invoke\n    return await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.InternalServerError: Error code: 503 - {'error': {'message': 'Service unavailable. Visit https://groqstatus.com/ to see if there is an active incident.', 'type': 'internal_server_error'}}\n",
    "source": "Error code: 503 - {'error': {'message': 'Service unavailable. Visit https://groqstatus.com/ to see if there is an active incident.', 'type': 'internal_server_error'}}",
    "details": {
        "prompt": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n",
        "kwargs": {
            "history": [
                {
                    "content": "\n-Goal-\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n \n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: One of the following types: [organization,person,geo,event]\n- entity_description: Comprehensive description of the entity's attributes and activities\nFormat each entity as (\"entity\"<|><entity_name><|><entity_type><|><entity_description>)\n \n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n Format each relationship as (\"relationship\"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n \n3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n \n4. When finished, output <|COMPLETE|>\n \n######################\n-Examples-\n######################\nExample 1:\nEntity_types: ORGANIZATION,PERSON\nText:\nThe Verdantis's Central Institution is scheduled to meet on Monday and Thursday, with the institution planning to release its latest policy decision on Thursday at 1:30 p.m. PDT, followed by a press conference where Central Institution Chair Martin Smith will take questions. Investors expect the Market Strategy Committee to hold its benchmark interest rate steady in a range of 3.5%-3.75%.\n######################\nOutput:\n(\"entity\"<|>CENTRAL INSTITUTION<|>ORGANIZATION<|>The Central Institution is the Federal Reserve of Verdantis, which is setting interest rates on Monday and Thursday)\n##\n(\"entity\"<|>MARTIN SMITH<|>PERSON<|>Martin Smith is the chair of the Central Institution)\n##\n(\"entity\"<|>MARKET STRATEGY COMMITTEE<|>ORGANIZATION<|>The Central Institution committee makes key decisions about interest rates and the growth of Verdantis's money supply)\n##\n(\"relationship\"<|>MARTIN SMITH<|>CENTRAL INSTITUTION<|>Martin Smith is the Chair of the Central Institution and will answer questions at a press conference<|>9)\n<|COMPLETE|>\n\n######################\nExample 2:\nEntity_types: ORGANIZATION\nText:\nTechGlobal's (TG) stock skyrocketed in its opening day on the Global Exchange Thursday. But IPO experts warn that the semiconductor corporation's debut on the public markets isn't indicative of how other newly listed companies may perform.\n\nTechGlobal, a formerly public company, was taken private by Vision Holdings in 2014. The well-established chip designer says it powers 85% of premium smartphones.\n######################\nOutput:\n(\"entity\"<|>TECHGLOBAL<|>ORGANIZATION<|>TechGlobal is a stock now listed on the Global Exchange which powers 85% of premium smartphones)\n##\n(\"entity\"<|>VISION HOLDINGS<|>ORGANIZATION<|>Vision Holdings is a firm that previously owned TechGlobal)\n##\n(\"relationship\"<|>TECHGLOBAL<|>VISION HOLDINGS<|>Vision Holdings formerly owned TechGlobal from 2014 until present<|>5)\n<|COMPLETE|>\n\n######################\nExample 3:\nEntity_types: ORGANIZATION,GEO,PERSON\nText:\nFive Aurelians jailed for 8 years in Firuzabad and widely regarded as hostages are on their way home to Aurelia.\n\nThe swap orchestrated by Quintara was finalized when $8bn of Firuzi funds were transferred to financial institutions in Krohaara, the capital of Quintara.\n\nThe exchange initiated in Firuzabad's capital, Tiruzia, led to the four men and one woman, who are also Firuzi nationals, boarding a chartered flight to Krohaara.\n\nThey were welcomed by senior Aurelian officials and are now on their way to Aurelia's capital, Cashion.\n\nThe Aurelians include 39-year-old businessman Samuel Namara, who has been held in Tiruzia's Alhamia Prison, as well as journalist Durke Bataglani, 59, and environmentalist Meggie Tazbah, 53, who also holds Bratinas nationality.\n######################\nOutput:\n(\"entity\"<|>FIRUZABAD<|>GEO<|>Firuzabad held Aurelians as hostages)\n##\n(\"entity\"<|>AURELIA<|>GEO<|>Country seeking to release hostages)\n##\n(\"entity\"<|>QUINTARA<|>GEO<|>Country that negotiated a swap of money in exchange for hostages)\n##\n##\n(\"entity\"<|>TIRUZIA<|>GEO<|>Capital of Firuzabad where the Aurelians were being held)\n##\n(\"entity\"<|>KROHAARA<|>GEO<|>Capital city in Quintara)\n##\n(\"entity\"<|>CASHION<|>GEO<|>Capital city in Aurelia)\n##\n(\"entity\"<|>SAMUEL NAMARA<|>PERSON<|>Aurelian who spent time in Tiruzia's Alhamia Prison)\n##\n(\"entity\"<|>ALHAMIA PRISON<|>GEO<|>Prison in Tiruzia)\n##\n(\"entity\"<|>DURKE BATAGLANI<|>PERSON<|>Aurelian journalist who was held hostage)\n##\n(\"entity\"<|>MEGGIE TAZBAH<|>PERSON<|>Bratinas national and environmentalist who was held hostage)\n##\n(\"relationship\"<|>FIRUZABAD<|>AURELIA<|>Firuzabad negotiated a hostage exchange with Aurelia<|>2)\n##\n(\"relationship\"<|>QUINTARA<|>AURELIA<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n(\"relationship\"<|>QUINTARA<|>FIRUZABAD<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n(\"relationship\"<|>SAMUEL NAMARA<|>ALHAMIA PRISON<|>Samuel Namara was a prisoner at Alhamia prison<|>8)\n##\n(\"relationship\"<|>SAMUEL NAMARA<|>MEGGIE TAZBAH<|>Samuel Namara and Meggie Tazbah were exchanged in the same hostage release<|>2)\n##\n(\"relationship\"<|>SAMUEL NAMARA<|>DURKE BATAGLANI<|>Samuel Namara and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n(\"relationship\"<|>MEGGIE TAZBAH<|>DURKE BATAGLANI<|>Meggie Tazbah and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n(\"relationship\"<|>SAMUEL NAMARA<|>FIRUZABAD<|>Samuel Namara was a hostage in Firuzabad<|>2)\n##\n(\"relationship\"<|>MEGGIE TAZBAH<|>FIRUZABAD<|>Meggie Tazbah was a hostage in Firuzabad<|>2)\n##\n(\"relationship\"<|>DURKE BATAGLANI<|>FIRUZABAD<|>Durke Bataglani was a hostage in Firuzabad<|>2)\n<|COMPLETE|>\n\n######################\n-Real Data-\n######################\nEntity_types: organization,person,geo,event\nText: , tRNAs, and various enzymatic factors.\n\nRibosomes\nEven before an mRNA is translated, a cell must invest energy to build each of its ribosomes. In E. coli, there are between 10,000 and 70,000 ribosomes present in each cell at any given time. A ribosome is a complex macromolecule composed of structural and catalytic rRNAs, and many distinct polypeptides. In eukaryotes, the nucleolus is completely specialized for the synthesis and assembly of rRNAs.\n\nRibosomes exist in the cytoplasm in prokaryotes and in the cytoplasm and rough endoplasmic reticulum in eukaryotes. Mitochondria and chloroplasts also have their own ribosomes in the matrix and stroma, which look more similar to prokaryotic ribosomes (and have similar drug sensitivities) than the ribosomes just outside their outer membranes in the cytoplasm. Ribosomes dissociate into large and small subunits when they are not synthesizing proteins and reassociate during the initiation of translation. In E. coli, the small subunit is described as 30S, and the large subunit is 50S, for a total of 70S (recall that Svedberg units are not additive). Mammalian ribosomes have a small 40S subunit and a large 60S subunit, for a total of 80S. The small subunit is responsible for binding the mRNA template, whereas the large subunit sequentially binds tRNAs. Each mRNA molecule is simultaneously translated by many ribosomes, all synthesizing protein in the same direction: reading the mRNA from 5' to 3' and synthesizing the polypeptide from the N terminus to the C terminus. The complete mRNA/poly-ribosome structure is called a polysome.\n\ntRNAs\nThe tRNAs are structural RNA molecules that were transcribed from genes by RNA polymerase III. Depending on the species, 40 to 60 types of tRNAs exist in the cytoplasm. Serving as adaptors, specific tRNAs bind to sequences on the mRNA template and add the corresponding amino acid to the polypeptide chain. Therefore, tRNAs are the molecules that actually “translate” the language of RNA into the language of proteins.\n\nOf the 64 possible mRNA codons—or triplet combinations of A, U, G, and C—three specify the termination of protein synthesis and 61 specify the addition of amino acids to the polypeptide chain. Of these 61, one codon (AUG) also encodes the initiation of translation. Each tRNA anticodon can base pair with one of the mRNA codons and add an amino acid or terminate translation, according to the genetic code. For instance, if the sequence CUA occurred on an mRNA template in the proper reading frame, it would bind a tRNA expressing the complementary sequence, GAU, which would be linked to the amino acid leucine.\n\nAs the adaptor molecules of translation, it is surprising that tRNAs can fit so much specificity into such a small package. Consider that tRNAs need to interact with three factors: 1) they must be recognized by the correct aminoacyl synthetase (see below); 2) they must be recognized by ribosomes; and 3) they must bind to the correct sequence in mRNA.\n\nAminoacyl tRNA Synthetases\nThe process of pre-tRNA synthesis by RNA polymerase III only creates the RNA portion of the adaptor molecule. The corresponding amino acid must be added later, once the tRNA is processed and exported to the cytoplasm. Through the process of tRNA “charging,” each tRNA molecule is linked to its correct amino acid by a group of enzymes called aminoacyl tRNA synthetases. At least one type of aminoacyl tRNA synthetase exists for each of the 20 amino acids; the exact number of aminoacyl tRNA synthetases varies by species. These enzymes first bind and hydrolyze ATP to catalyze a high-energy bond between an amino acid and adenosine monophosphate (AMP); a pyrophosphate molecule is expelled in this reaction. The activated amino acid is then transferred to the tRNA, and AMP is released.\n\nThe Mechanism of Protein Synthesis\nAs with mRNA synthesis, protein synthesis can be divided into three phases: initiation, elongation, and termination. The process of translation is similar in prokaryotes and eukaryotes. Here we’ll explore how translation occurs in E. coli, a representative prokaryote, and specify any differences between prokaryotic and eukaryotic translation.\n\nInitiation of Translation\nProtein synthesis begins with the formation of an initiation complex. In E. coli, this complex involves the small 30S ribosome, the mRNA template, three initiation factors (IFs; IF-1, IF-2, and IF-3), and a special initiator tRNA, called  tRNAMetf\n . The initiator tRNA interacts with the start codon AUG (or rarely, GUG), links to a formylated methionine called fMet, and can also bind IF-2. Formylated methionine is inserted by  fMet−tRNAMetf\n  at the beginning of every polypeptide chain synthesized by E. coli, but it is usually clipped off after translation is complete. When an in-frame AUG is encountered during translation elongation, a non-formylated methionine is inserted by a regular Met-tRNAMet.\n\nIn E. coli mRNA, a sequence upstream of the first AUG codon, called the Shine-Dalgarno sequence (AGGAGG), interacts with the rRNA\n######################\nOutput:",
                    "role": "user"
                },
                {
                    "role": "assistant",
                    "content": "Here is the output:\n\n(\"entity\"<|>RIBOSOMES<|>ORGANIZATION<|>Ribosomes are complex macromolecules composed of structural and catalytic rRNAs, and many distinct polypeptides)\n##\n(\"entity\"<|>E. COLI<|>ORGANIZATION<|>E. coli is a type of bacteria with 10,000 to 70,000 ribosomes present in each cell)\n##\n(\"entity\"<|>NUCLEOLUS<|>GEO<|>Nucleolus is a region in eukaryotes specialized for the synthesis and assembly of rRNAs)\n##\n(\"entity\"<|>MITOCHONDRIA<|>GEO<|>Mitochondria have their own ribosomes in the matrix and stroma)\n##\n(\"entity\"<|>CHLOROPLASTS<|>GEO<|>Chloroplasts have their own ribosomes in the matrix and stroma)\n##\n(\"entity\"<|>CYTOPLASM<|>GEO<|>Cytoplasm is the region in cells where ribosomes exist)\n##\n(\"entity\"<|>ROUGH ENDOPLASMIC RETICULUM<|>GEO<|>Rough endoplasmic reticulum is a region in eukaryotes where ribosomes exist)\n##\n(\"entity\"<|>RNA POLYMERASE III<|>ORGANIZATION<|>RNA polymerase III is an enzyme that transcribes genes into tRNAs)\n##\n(\"entity\"<|>tRNAs<|>ORGANIZATION<|>tRNAs are structural RNA molecules that translate the language of RNA into the language of proteins)\n##\n(\"entity\"<|>AMINOACYL tRNA SYNTHETASES<|>ORGANIZATION<|>Aminoacyl tRNA synthetases are enzymes that link tRNAs to their correct amino acids)\n##\n(\"entity\"<|>IF-1<|>PERSON<|>IF-1 is an initiation factor involved in the initiation of translation)\n##\n(\"entity\"<|>IF-2<|>PERSON<|>IF-2 is an initiation factor involved in the initiation of translation)\n##\n(\"entity\"<|>IF-3<|>PERSON<|>IF-3 is an initiation factor involved in the initiation of translation)\n##\n(\"entity\"<|>tRNAMetf<|>ORGANIZATION<|>tRNAMetf is a special initiator tRNA involved in the initiation of translation)\n##\n(\"relationship\"<|>RIBOSOMES<|>E. COLI<|>Ribosomes exist in E. coli cells<|>8)\n##\n(\"relationship\"<|>NUCLEOLUS<|>RIBOSOMES<|>Nucleolus is involved in the synthesis and assembly of ribosomes<|>5)\n##\n(\"relationship\"<|>MITOCHONDRIA<|>RIBOSOMES<|>Mitochondria have their own ribosomes<|>5)\n##\n(\"relationship\"<|>CHLOROPLASTS<|>RIBOSOMES<|>Chloroplasts have their own ribosomes<|>5)\n##\n(\"relationship\"<|>RNA POLYMERASE III<|>tRNAs<|>RNA polymerase III transcribes genes into tRNAs<|>8)\n##\n(\"relationship\"<|>AMINOACYL tRNA SYNTHETASES<|>tRNAs<|>Aminoacyl tRNA synthetases link tRNAs to their correct amino acids<|>8)\n##\n(\"relationship\"<|>IF-1<|>tRNAMetf<|>IF-1 interacts with tRNAMetf during initiation of translation<|>5)\n##\n(\"relationship\"<|>IF-2<|>tRNAMetf<|>IF-2 interacts with tRNAMetf during initiation of translation<|>5)\n##\n(\"relationship\"<|>IF-3<|>tRNAMetf<|>IF-3 interacts with tRNAMetf during initiation of translation<|>5)\n##\n<|COMPLETE|>"
                }
            ],
            "name": "extract-continuation-0"
        }
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/extract_graph/graph_extractor.py\", line 118, in __call__\n    result = await self._process_document(text, prompt_variables)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/extract_graph/graph_extractor.py\", line 158, in _process_document\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 84, in achat\n    response = await self.model(prompt, history=history, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 78, in invoke\n    return await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.InternalServerError: Error code: 503 - {'error': {'message': 'Service unavailable. Visit https://groqstatus.com/ to see if there is an active incident.', 'type': 'internal_server_error'}}\n",
    "source": "Error code: 503 - {'error': {'message': 'Service unavailable. Visit https://groqstatus.com/ to see if there is an active incident.', 'type': 'internal_server_error'}}",
    "details": {
        "doc_index": 0,
        "text": ", tRNAs, and various enzymatic factors.\n\nRibosomes\nEven before an mRNA is translated, a cell must invest energy to build each of its ribosomes. In E. coli, there are between 10,000 and 70,000 ribosomes present in each cell at any given time. A ribosome is a complex macromolecule composed of structural and catalytic rRNAs, and many distinct polypeptides. In eukaryotes, the nucleolus is completely specialized for the synthesis and assembly of rRNAs.\n\nRibosomes exist in the cytoplasm in prokaryotes and in the cytoplasm and rough endoplasmic reticulum in eukaryotes. Mitochondria and chloroplasts also have their own ribosomes in the matrix and stroma, which look more similar to prokaryotic ribosomes (and have similar drug sensitivities) than the ribosomes just outside their outer membranes in the cytoplasm. Ribosomes dissociate into large and small subunits when they are not synthesizing proteins and reassociate during the initiation of translation. In E. coli, the small subunit is described as 30S, and the large subunit is 50S, for a total of 70S (recall that Svedberg units are not additive). Mammalian ribosomes have a small 40S subunit and a large 60S subunit, for a total of 80S. The small subunit is responsible for binding the mRNA template, whereas the large subunit sequentially binds tRNAs. Each mRNA molecule is simultaneously translated by many ribosomes, all synthesizing protein in the same direction: reading the mRNA from 5' to 3' and synthesizing the polypeptide from the N terminus to the C terminus. The complete mRNA/poly-ribosome structure is called a polysome.\n\ntRNAs\nThe tRNAs are structural RNA molecules that were transcribed from genes by RNA polymerase III. Depending on the species, 40 to 60 types of tRNAs exist in the cytoplasm. Serving as adaptors, specific tRNAs bind to sequences on the mRNA template and add the corresponding amino acid to the polypeptide chain. Therefore, tRNAs are the molecules that actually “translate” the language of RNA into the language of proteins.\n\nOf the 64 possible mRNA codons—or triplet combinations of A, U, G, and C—three specify the termination of protein synthesis and 61 specify the addition of amino acids to the polypeptide chain. Of these 61, one codon (AUG) also encodes the initiation of translation. Each tRNA anticodon can base pair with one of the mRNA codons and add an amino acid or terminate translation, according to the genetic code. For instance, if the sequence CUA occurred on an mRNA template in the proper reading frame, it would bind a tRNA expressing the complementary sequence, GAU, which would be linked to the amino acid leucine.\n\nAs the adaptor molecules of translation, it is surprising that tRNAs can fit so much specificity into such a small package. Consider that tRNAs need to interact with three factors: 1) they must be recognized by the correct aminoacyl synthetase (see below); 2) they must be recognized by ribosomes; and 3) they must bind to the correct sequence in mRNA.\n\nAminoacyl tRNA Synthetases\nThe process of pre-tRNA synthesis by RNA polymerase III only creates the RNA portion of the adaptor molecule. The corresponding amino acid must be added later, once the tRNA is processed and exported to the cytoplasm. Through the process of tRNA “charging,” each tRNA molecule is linked to its correct amino acid by a group of enzymes called aminoacyl tRNA synthetases. At least one type of aminoacyl tRNA synthetase exists for each of the 20 amino acids; the exact number of aminoacyl tRNA synthetases varies by species. These enzymes first bind and hydrolyze ATP to catalyze a high-energy bond between an amino acid and adenosine monophosphate (AMP); a pyrophosphate molecule is expelled in this reaction. The activated amino acid is then transferred to the tRNA, and AMP is released.\n\nThe Mechanism of Protein Synthesis\nAs with mRNA synthesis, protein synthesis can be divided into three phases: initiation, elongation, and termination. The process of translation is similar in prokaryotes and eukaryotes. Here we’ll explore how translation occurs in E. coli, a representative prokaryote, and specify any differences between prokaryotic and eukaryotic translation.\n\nInitiation of Translation\nProtein synthesis begins with the formation of an initiation complex. In E. coli, this complex involves the small 30S ribosome, the mRNA template, three initiation factors (IFs; IF-1, IF-2, and IF-3), and a special initiator tRNA, called  tRNAMetf\n . The initiator tRNA interacts with the start codon AUG (or rarely, GUG), links to a formylated methionine called fMet, and can also bind IF-2. Formylated methionine is inserted by  fMet−tRNAMetf\n  at the beginning of every polypeptide chain synthesized by E. coli, but it is usually clipped off after translation is complete. When an in-frame AUG is encountered during translation elongation, a non-formylated methionine is inserted by a regular Met-tRNAMet.\n\nIn E. coli mRNA, a sequence upstream of the first AUG codon, called the Shine-Dalgarno sequence (AGGAGG), interacts with the rRNA"
    }
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499168, Requested 2318. Please try again in 4m16.704799999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499167, Requested 2738. Please try again in 5m29.1368s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499062, Requested 2576. Please try again in 4m42.9142s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498844, Requested 3816. Please try again in 7m39.5202s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498717, Requested 2296. Please try again in 2m54.9432s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498587, Requested 2338. Please try again in 2m39.7368s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498418, Requested 3043. Please try again in 4m12.4208s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498233, Requested 3310. Please try again in 4m26.470399999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498095, Requested 2534. Please try again in 1m48.6286s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 497960, Requested 2459. Please try again in 1m12.3216s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 497795, Requested 2935. Please try again in 2m6.0024s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 497629, Requested 2980. Please try again in 1m45.1594s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500129, Requested 2537. Please try again in 7m40.7436s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499971, Requested 2343. Please try again in 6m39.8384s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499825, Requested 3207. Please try again in 8m43.867599999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499629, Requested 3687. Please try again in 9m32.8706s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499497, Requested 2360. Please try again in 5m20.825s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499346, Requested 2704. Please try again in 5m54.230199999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499172, Requested 3238. Please try again in 6m56.403399999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499039, Requested 2376. Please try again in 4m4.503799999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498907, Requested 2378. Please try again in 3m41.986399999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498767, Requested 2505. Please try again in 3m39.683s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498615, Requested 2814. Please try again in 4m6.7702s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498479, Requested 2460. Please try again in 2m42.232s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498304, Requested 3204. Please try again in 4m20.5702s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498164, Requested 2556. Please try again in 2m4.2688s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500057, Requested 2831. Please try again in 8m19.146199999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500057, Requested 2530. Please try again in 7m27.067399999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499913, Requested 2921. Please try again in 8m9.626199999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499772, Requested 2992. Please try again in 7m57.489s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499609, Requested 2942. Please try again in 7m20.796s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499441, Requested 3157. Please try again in 7m28.767s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499294, Requested 2644. Please try again in 5m34.736599999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499145, Requested 2709. Please try again in 5m20.2496s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499018, Requested 2291. Please try again in 3m46.1052s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498866, Requested 2617. Please try again in 4m16.173s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498701, Requested 2981. Please try again in 4m50.6282s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498544, Requested 2925. Please try again in 4m13.7334s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498407, Requested 2466. Please try again in 2m30.8472s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498242, Requested 3081. Please try again in 3m48.4742s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498091, Requested 2801. Please try again in 2m33.9922s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 497954, Requested 2544. Please try again in 1m25.9016s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 497748, Requested 3808. Please try again in 4m28.8458s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 497587, Requested 2929. Please try again in 1m29.100599999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499838, Requested 2987. Please try again in 8m8.0132s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499705, Requested 2371. Please try again in 5m58.578399999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499576, Requested 2330. Please try again in 5m29.3146s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499403, Requested 3216. Please try again in 7m32.5164s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499227, Requested 3313. Please try again in 7m18.787s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499097, Requested 2360. Please try again in 4m11.6346s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498958, Requested 2520. Please try again in 4m15.2506s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498831, Requested 2284. Please try again in 3m12.5938s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498662, Requested 3106. Please try again in 5m5.4054s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498513, Requested 2685. Please try again in 3m27.0056s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498364, Requested 2719. Please try again in 3m6.9718s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498233, Requested 2353. Please try again in 1m41.174s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498096, Requested 2528. Please try again in 1m47.691s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 497942, Requested 2901. Please try again in 2m25.5284s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499999, Requested 2292. Please try again in 6m35.8088s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499999, Requested 3083. Please try again in 8m52.4276s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499796, Requested 3983. Please try again in 10m52.8796s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499668, Requested 2319. Please try again in 5m43.1884s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499534, Requested 2396. Please try again in 5m33.483s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499390, Requested 2656. Please try again in 5m53.489s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499241, Requested 2685. Please try again in 5m32.8062s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499033, Requested 3885. Please try again in 8m24.1212s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498895, Requested 2523. Please try again in 4m4.885599999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498636, Requested 4646. Please try again in 9m27.015s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498387, Requested 4395. Please try again in 8m0.635199999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498244, Requested 2535. Please try again in 2m14.477199999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 497958, Requested 5258. Please try again in 9m15.612599999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 497717, Requested 4377. Please try again in 6m1.7278s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 497420, Requested 4707. Please try again in 6m7.4308s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500108, Requested 5312. Please try again in 15m36.7226s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499932, Requested 3177. Please try again in 8m57.1396s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499619, Requested 5684. Please try again in 15m16.3352s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499446, Requested 3203. Please try again in 7m37.656399999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499117, Requested 5907. Please try again in 14m27.9816s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498856, Requested 4856. Please try again in 10m41.365799999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498604, Requested 4162. Please try again in 7m57.8586s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498263, Requested 6779. Please try again in 14m31.1502s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 144, in __call__\n    return await self._decorated_target(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 77, in invoke\n    return await this.invoke_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 96, in invoke_json\n    return await self.try_receive_json(delegate, prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/json.py\", line 112, in try_receive_json\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/cached.py\", line 115, in invoke\n    result = await delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/services/rate_limiter.py\", line 75, in invoke\n    result = await delegate(prompt, **args)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 126, in _decorator_target\n    output = await self._execute_llm(prompt, kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_text_chat_llm.py\", line 157, in _execute_llm\n    completion = await self._client.chat.completions.create(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jss0mqvces98cxfmen4f1jpd` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 498006, Requested 4627. Please try again in 7m34.8966s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/index/operations/summarize_communities/community_reports_extractor.py\", line 80, in __call__\n    response = await self._model.achat(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/models.py\", line 82, in achat\n    response = await self.model(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/llm/openai_chat_llm.py\", line 94, in __call__\n    return await self._text_chat_llm(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/openai/services/openai_tools_parsing.py\", line 130, in __call__\n    return await self._delegate(prompt, **kwargs)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/fnllm/base/base_llm.py\", line 148, in __call__\n    await self._events.on_error(\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/events.py\", line 26, in on_error\n    self._on_error(error, traceback, arguments)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/language_model/providers/fnllm/utils.py\", line 45, in on_error\n    callbacks.error(\"Error Invoking LLM\", error, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/workflow_callbacks_manager.py\", line 64, in error\n    callback.error(message, cause, stack, details)\n  File \"/home/macierz/s184306/praca_magisterska/praca_magisterska/method2_graphrag/venv/lib/python3.10/site-packages/graphrag/callbacks/file_workflow_callbacks.py\", line 37, in error\n    json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 201, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n    o = _default(o)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 179, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type ModelMetaclass is not JSON serializable\n",
    "source": "Object of type ModelMetaclass is not JSON serializable",
    "details": null
}

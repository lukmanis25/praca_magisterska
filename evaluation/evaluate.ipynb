{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d84673aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macierz/s184306/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from ragas.metrics import answer_relevancy, answer_correctness, FactualCorrectness, SemanticSimilarity\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_localai import LocalAIEmbeddings\n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa962040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55732/1901439954.py:2: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_base=os.getenv(\"OPENAI_API_BASE\"), \n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),              \n",
    "    model_name=os.getenv(\"LLM_MODEL_NAME\")          \n",
    ")\n",
    "llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "embedding = LocalAIEmbeddings(\n",
    "    openai_api_base=os.getenv(\"EMBED_URL\"), \n",
    "    openai_api_key=os.getenv(\"EMBED_TOKEN\"), \n",
    "    model=os.getenv(\"EMBED_MODEL\")\n",
    ")\n",
    "embedding = LangchainEmbeddingsWrapper(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c59d835",
   "metadata": {},
   "source": [
    "# First Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537bcfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_evaluate(test_set_file, output_file):\n",
    "    with open(test_set_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        existing_df = pd.read_csv(output_file)\n",
    "    else:\n",
    "        existing_df = pd.DataFrame(columns=[\n",
    "            'user_input', 'response', 'reference',\n",
    "            'answer_correctness', 'answer_relevancy'\n",
    "        ])\n",
    "\n",
    "    existing_df.fillna('', inplace=True)\n",
    "\n",
    "    for idx, (q, a, g) in enumerate(zip(data['question'], data['answer'], data['ground_truth']), start=1):\n",
    "        short_q = q[:50].replace('\\n', ' ')\n",
    "        print(f\"[{idx}] Processing question: \\\"{short_q}...\\\"\")\n",
    "        if a == \"x\":\n",
    "            print(f\"[{idx}] Skipped (x in ans)\")\n",
    "            continue\n",
    "        match = existing_df[\n",
    "            (existing_df['user_input'] == q) &\n",
    "            (existing_df['response'] == a) &\n",
    "            (existing_df['reference'] == g)\n",
    "        ]\n",
    "        if not match.empty:\n",
    "            existing_row = match.iloc[0]\n",
    "            correctness = pd.to_numeric(existing_row['answer_correctness'], errors='coerce')\n",
    "            relevancy = pd.to_numeric(existing_row['answer_relevancy'], errors='coerce')\n",
    "            if (\n",
    "                not pd.isna(correctness) and not pd.isna(relevancy) and\n",
    "                correctness > 0.0 and relevancy > 0.0\n",
    "            ):\n",
    "                print(f\"[{idx}] Skipped (already evaluated with valid metrics)\")\n",
    "                continue\n",
    "\n",
    "        print(f\"[{idx}] Running evaluation...\")\n",
    "        single_data = {\n",
    "            \"question\": [q],\n",
    "            \"answer\": [a],\n",
    "            \"ground_truth\": [g]\n",
    "        }\n",
    "        single_dataset = Dataset.from_dict(single_data)\n",
    "\n",
    "        try:\n",
    "            results = evaluate(\n",
    "                dataset=single_dataset,\n",
    "                metrics=[answer_correctness, answer_relevancy],\n",
    "                llm=llm,\n",
    "                embeddings=embedding\n",
    "            )\n",
    "            row_df = results.to_pandas()\n",
    "            print(f\"[{idx}] Evaluation completed successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{idx}] Evaluation error: {e}\")\n",
    "            row_df = pd.DataFrame([{\n",
    "                'user_input': q,\n",
    "                'response': a,\n",
    "                'reference': g,\n",
    "                'answer_correctness': 0.0,\n",
    "                'answer_relevancy': 0.0\n",
    "            }])\n",
    "\n",
    "        existing_df = pd.concat([existing_df, row_df], ignore_index=True)\n",
    "        existing_df.drop_duplicates(\n",
    "            subset=['user_input', 'response', 'reference'], keep='last', inplace=True\n",
    "        )\n",
    "        existing_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"All done — results saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af39f85a",
   "metadata": {},
   "source": [
    "## Evaluation with meta-llama/llama-4-scout-17b-16e-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47ec78f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_base=os.getenv(\"OPENAI_API_BASE\"), \n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),              \n",
    "    model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\"         \n",
    ")\n",
    "llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "embedding = LocalAIEmbeddings(\n",
    "    openai_api_base=os.getenv(\"EMBED_URL\"), \n",
    "    openai_api_key=os.getenv(\"EMBED_TOKEN\"), \n",
    "    model=os.getenv(\"EMBED_MODEL\")\n",
    ")\n",
    "embedding = LangchainEmbeddingsWrapper(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4df1a4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/method1_lightrag_hybrid.csv\n",
      "Average answer correctness: 0.5122069836697539\n",
      "Average answer relevancy: 0.6982478951966491\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method1_lightrag_hybrid.json'\n",
    "output_file = './results/study_rules/method1_lightrag_hybrid.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_lr_hybrid = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_lr_hybrid['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_lr_hybrid['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3ef078b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/method1_lightrag_local.csv\n",
      "Average answer correctness: 0.4249733737262503\n",
      "Average answer relevancy: 0.6681967775090379\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method1_lightrag_local.json'\n",
    "output_file = './results/study_rules/method1_lightrag_local.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_lr_local = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_lr_local['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_lr_local['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6762cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/method2_graphrag_drift.csv\n",
      "Average answer correctness: 0.3500237400348909\n",
      "Average answer relevancy: 0.7774822101039724\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method2_graphrag_drift.json'\n",
    "output_file = './results/study_rules/method2_graphrag_drift.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_gr_drift = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_gr_drift['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_gr_drift['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bff0d871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/naive_rag.csv\n",
      "Average answer correctness: 0.5506225438324175\n",
      "Average answer relevancy: 0.7527904156645951\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_naive_rag.json'\n",
    "output_file = './results/study_rules/naive_rag.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_naive = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_naive['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_naive['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38a2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/naive_rag_small_top_k.csv\n",
      "Average answer correctness: 0.5652947016792146\n",
      "Average answer relevancy: 0.6714817258125219\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_naive_rag_small_top_k.json'\n",
    "output_file = './results/study_rules/naive_rag_small_top_k.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_naive_small_k = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_naive_small_k['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_naive_small_k['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0821265",
   "metadata": {},
   "source": [
    "## Evaluation with deepseek-r1-distill-llama-70b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d1518f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_base=os.getenv(\"OPENAI_API_BASE\"), \n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),              \n",
    "    model_name=\"deepseek-r1-distill-llama-70b\"         \n",
    ")\n",
    "llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "embedding = LocalAIEmbeddings(\n",
    "    openai_api_base=os.getenv(\"EMBED_URL\"), \n",
    "    openai_api_key=os.getenv(\"EMBED_TOKEN\"), \n",
    "    model=os.getenv(\"EMBED_MODEL\")\n",
    ")\n",
    "embedding = LangchainEmbeddingsWrapper(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "087bd949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/method1_lightrag_hybrid_deepseek.csv\n",
      "Average answer correctness: 0.49797992824280307\n",
      "Average answer relevancy: 0.7349126593819586\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method1_lightrag_hybrid.json'\n",
    "output_file = './results/study_rules/method1_lightrag_hybrid_deepseek.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_lr_hybrid_deepseek = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_lr_hybrid_deepseek['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_lr_hybrid_deepseek['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c04f9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/method1_lightrag_local_deepseek.csv\n",
      "Average answer correctness: 0.4016634365973334\n",
      "Average answer relevancy: 0.5804095482320186\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method1_lightrag_local.json'\n",
    "output_file = './results/study_rules/method1_lightrag_local_deepseek.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_lr_local_deepseek = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_lr_local_deepseek['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_lr_local_deepseek['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80a1c313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/method2_graphrag_drift_deepseek.csv\n",
      "Average answer correctness: 0.39543015771151035\n",
      "Average answer relevancy: 0.7339491973490562\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method2_graphrag_drift.json'\n",
    "output_file = './results/study_rules/method2_graphrag_drift_deepseek.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_gr_drift_deepseek = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_gr_drift_deepseek['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_gr_drift_deepseek['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6df536c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/naive_rag_deepseek.csv\n",
      "Average answer correctness: 0.5585839181481368\n",
      "Average answer relevancy: 0.7402449547731885\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_naive_rag.json'\n",
    "output_file = './results/study_rules/naive_rag_deepseek.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_naive_deepseek = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_naive_deepseek['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_naive_deepseek['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1811176d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/naive_rag_small_top_k_deepseek.csv\n",
      "Average answer correctness: 0.5452149662099343\n",
      "Average answer relevancy: 0.6653694964764589\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_naive_rag_small_top_k.json'\n",
    "output_file = './results/study_rules/naive_rag_small_top_k_deepseek.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_naive_small_k_deepseek = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_naive_small_k_deepseek['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_naive_small_k_deepseek['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deccca0",
   "metadata": {},
   "source": [
    "# Second Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fc59532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_evaluate_2(test_set_file, output_file):\n",
    "    with open(test_set_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        existing_df = pd.read_csv(output_file)\n",
    "    else:\n",
    "        existing_df = pd.DataFrame(columns=[\n",
    "            'user_input', 'response', 'reference',\n",
    "            'answer_correctness', 'answer_relevancy', 'factual_correctness(mode=f1)',\n",
    "            'factual_correctness(mode=recall)', 'semantic_similarity'\n",
    "        ])\n",
    "\n",
    "    existing_df.fillna('', inplace=True)\n",
    "\n",
    "    for idx, (q, a, g) in enumerate(zip(data['question'], data['answer'], data['ground_truth']), start=1):\n",
    "        short_q = q[:50].replace('\\n', ' ')\n",
    "        print(f\"[{idx}] Processing question: \\\"{short_q}...\\\"\")\n",
    "        if a == \"x\":\n",
    "            print(f\"[{idx}] Skipped (x in ans)\")\n",
    "            continue\n",
    "        match = existing_df[\n",
    "            (existing_df['user_input'] == q) &\n",
    "            (existing_df['response'] == a) &\n",
    "            (existing_df['reference'] == g)\n",
    "        ]\n",
    "        if not match.empty:\n",
    "            existing_row = match.iloc[0]\n",
    "            correctness = pd.to_numeric(existing_row['answer_correctness'], errors='coerce')\n",
    "            relevancy = pd.to_numeric(existing_row['answer_relevancy'], errors='coerce')\n",
    "            factual_correctness = pd.to_numeric(existing_row['factual_correctness(mode=f1)'], errors='coerce')\n",
    "            factual_correctness_recall = pd.to_numeric(existing_row['factual_correctness(mode=recall)'], errors='coerce')\n",
    "            semantic_similarity = pd.to_numeric(existing_row['semantic_similarity'], errors='coerce')\n",
    "            if (\n",
    "                not pd.isna(correctness) and not pd.isna(relevancy) and not pd.isna(factual_correctness) \n",
    "                and not pd.isna(factual_correctness_recall) and not pd.isna(semantic_similarity) and \n",
    "                correctness > 0.0 and relevancy > 0.0 and factual_correctness > 0.0  and factual_correctness_recall > 0.0\n",
    "                and semantic_similarity > 0.0\n",
    "            ):\n",
    "                print(f\"[{idx}] Skipped (already evaluated with valid metrics)\")\n",
    "                continue\n",
    "\n",
    "        print(f\"[{idx}] Running evaluation...\")\n",
    "        single_data = {\n",
    "            \"question\": [q],\n",
    "            \"answer\": [a],\n",
    "            \"ground_truth\": [g]\n",
    "        }\n",
    "        single_dataset = Dataset.from_dict(single_data)\n",
    "\n",
    "        try:\n",
    "            results = evaluate(\n",
    "                dataset=single_dataset,\n",
    "                metrics=[answer_correctness, answer_relevancy, FactualCorrectness(),\n",
    "                         FactualCorrectness(mode=\"recall\"), SemanticSimilarity()],\n",
    "                llm=llm,\n",
    "                embeddings=embedding\n",
    "            )\n",
    "            # results = evaluate(\n",
    "            #     dataset=single_dataset,\n",
    "            #     metrics=[answer_correctness, answer_relevancy, FactualCorrectness(atomicity=\"high\", coverage=\"high\"),\n",
    "            #              FactualCorrectness(mode=\"recall\",atomicity=\"high\", coverage=\"high\"), SemanticSimilarity()],\n",
    "            #     llm=llm,\n",
    "            #     embeddings=embedding\n",
    "            # )\n",
    "            print(results)\n",
    "            row_df = results.to_pandas()\n",
    "            print(f\"[{idx}] Evaluation completed successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{idx}] Evaluation error: {e}\")\n",
    "            row_df = pd.DataFrame([{\n",
    "                'user_input': q,\n",
    "                'response': a,\n",
    "                'reference': g,\n",
    "                'answer_correctness': 0.0,\n",
    "                'answer_relevancy': 0.0,\n",
    "                'factual_correctness(mode=f1)': 0.0,\n",
    "                'factual_correctness(mode=recall)': 0.0,\n",
    "                'semantic_similarity': 0.0\n",
    "            }])\n",
    "\n",
    "        existing_df = pd.concat([existing_df, row_df], ignore_index=True)\n",
    "        existing_df.drop_duplicates(\n",
    "            subset=['user_input', 'response', 'reference'], keep='last', inplace=True\n",
    "        )\n",
    "        existing_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"All done — results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5821634d",
   "metadata": {},
   "source": [
    "## Study Rules Evaluation with meta-llama/llama-4-scout-17b-16e-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6037f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_base=os.getenv(\"OPENAI_API_BASE\"), \n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),              \n",
    "    model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\"         \n",
    ")\n",
    "llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "embedding = LocalAIEmbeddings(\n",
    "    openai_api_base=os.getenv(\"EMBED_URL\"), \n",
    "    openai_api_key=os.getenv(\"EMBED_TOKEN\"), \n",
    "    model=os.getenv(\"EMBED_MODEL\")\n",
    ")\n",
    "embedding = LangchainEmbeddingsWrapper(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a1f993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules_2/method1_lightrag_hybrid.csv\n",
      "Average answer correctness: 0.5093708492159725\n",
      "Average answer relevancy: 0.6982478951966493\n",
      "Average factual correctness(mode=f1): 0.36714285714285716\n",
      "Average factual correctness(mode=recall): 0.3614285714285715\n",
      "Average semantic similarity: 0.7553770316263085\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method1_lightrag_hybrid.json'\n",
    "output_file = './results/study_rules_2/method1_lightrag_hybrid.csv'\n",
    "\n",
    "#results = start_evaluate_2(test_set_file, output_file)\n",
    "\n",
    "df_lr_hybrid2 = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_lr_hybrid2['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_lr_hybrid2['answer_relevancy'].mean())\n",
    "print(\"Average factual correctness(mode=f1):\", df_lr_hybrid2['factual_correctness(mode=f1)'].mean())\n",
    "print(\"Average factual correctness(mode=recall):\", df_lr_hybrid2['factual_correctness(mode=recall)'].mean())\n",
    "print(\"Average semantic similarity:\", df_lr_hybrid2['semantic_similarity'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b773ac51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules_2/method1_lightrag_local.csv\n",
      "Average answer correctness: 0.4249733737262503\n",
      "Average answer relevancy: 0.6681967775090379\n",
      "Average factual correctness(mode=f1): 0.2865\n",
      "Average factual correctness(mode=recall): 0.2575\n",
      "Average semantic similarity: 0.7379938908387216\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method1_lightrag_local.json'\n",
    "output_file = './results/study_rules_2/method1_lightrag_local.csv'\n",
    "\n",
    "#results = start_evaluate_2(test_set_file, output_file)\n",
    "\n",
    "df_lr_local2 = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_lr_local2['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_lr_local2['answer_relevancy'].mean())\n",
    "print(\"Average factual correctness(mode=f1):\", df_lr_local2['factual_correctness(mode=f1)'].mean())\n",
    "print(\"Average factual correctness(mode=recall):\", df_lr_local2['factual_correctness(mode=recall)'].mean())\n",
    "print(\"Average semantic similarity:\", df_lr_local2['semantic_similarity'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7c320bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules_2/method2_graphrag_drift.csv\n",
      "Average answer correctness: 0.3500237400348909\n",
      "Average answer relevancy: 0.7774822101039724\n",
      "Average factual correctness(mode=f1): 0.2935\n",
      "Average factual correctness(mode=recall): 0.34400000000000003\n",
      "Average semantic similarity: 0.7109516323324775\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method2_graphrag_drift.json'\n",
    "output_file = './results/study_rules_2/method2_graphrag_drift.csv'\n",
    "\n",
    "#results = start_evaluate_2(test_set_file, output_file)\n",
    "\n",
    "df_gr_drift2 = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_gr_drift2['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_gr_drift2['answer_relevancy'].mean())\n",
    "print(\"Average factual correctness(mode=f1):\", df_gr_drift2['factual_correctness(mode=f1)'].mean())\n",
    "print(\"Average factual correctness(mode=recall):\", df_gr_drift2['factual_correctness(mode=recall)'].mean())\n",
    "print(\"Average semantic similarity:\", df_gr_drift2['semantic_similarity'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cdd0561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules_2/naive_rag.csv\n",
      "Average answer correctness: 0.5502497388212984\n",
      "Average answer relevancy: 0.7530999940893139\n",
      "Average factual correctness(mode=f1): 0.4615\n",
      "Average factual correctness(mode=recall): 0.43050000000000005\n",
      "Average semantic similarity: 0.7995977826213055\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_naive_rag.json'\n",
    "output_file = './results/study_rules_2/naive_rag.csv'\n",
    "\n",
    "#results = start_evaluate_2(test_set_file, output_file)\n",
    "\n",
    "df_naive2 = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_naive2['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_naive2['answer_relevancy'].mean())\n",
    "print(\"Average factual correctness(mode=f1):\", df_naive2['factual_correctness(mode=f1)'].mean())\n",
    "print(\"Average factual correctness(mode=recall):\", df_naive2['factual_correctness(mode=recall)'].mean())\n",
    "print(\"Average semantic similarity:\", df_naive2['semantic_similarity'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec8949f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules_2/naive_rag_small_top_k.csv\n",
      "Average answer correctness: 0.5536140471861982\n",
      "Average answer relevancy: 0.6617379602286304\n",
      "Average factual correctness(mode=f1): 0.45050000000000007\n",
      "Average factual correctness(mode=recall): 0.4425\n",
      "Average semantic similarity: 0.7969714609933498\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_naive_rag_small_top_k.json'\n",
    "output_file = './results/study_rules_2/naive_rag_small_top_k.csv'\n",
    "\n",
    "#results = start_evaluate_2(test_set_file, output_file)\n",
    "\n",
    "df_naive_small_k2 = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_naive_small_k2['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_naive_small_k2['answer_relevancy'].mean())\n",
    "print(\"Average factual correctness(mode=f1):\", df_naive_small_k2['factual_correctness(mode=f1)'].mean())\n",
    "print(\"Average factual correctness(mode=recall):\", df_naive_small_k2['factual_correctness(mode=recall)'].mean())\n",
    "print(\"Average semantic similarity:\", df_naive_small_k2['semantic_similarity'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd70f5c",
   "metadata": {},
   "source": [
    "## Genetics Evaluation with meta-llama/llama-4-scout-17b-16e-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bfa9055",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55732/4194908582.py:14: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  existing_df.fillna('', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Processing question: \"Explain Griffith's transformation experiments. Wha...\"\n",
      "[1] Skipped (already evaluated with valid metrics)\n",
      "[2] Processing question: \"Why were radioactive sulfur and phosphorous used t...\"\n",
      "[2] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]Exception raised in Job[2]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500907, Requested 678. Please try again in 4m34.0008s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  40%|████      | 2/5 [01:05<01:38, 32.82s/it]Exception raised in Job[1]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500844, Requested 671. Please try again in 4m21.891199999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  60%|██████    | 3/5 [01:16<00:47, 23.69s/it]Exception raised in Job[0]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500432, Requested 776. Please try again in 3m28.875199999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  80%|████████  | 4/5 [02:27<00:41, 41.40s/it]Exception raised in Job[3]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500274, Requested 678. Please try again in 2m44.5338s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating: 100%|██████████| 5/5 [02:55<00:00, 35.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': nan, 'answer_relevancy': nan, 'factual_correctness(mode=f1)': nan, 'factual_correctness(mode=recall)': nan, 'semantic_similarity': 0.5869}\n",
      "[2] Evaluation completed successfully.\n",
      "[3] Processing question: \"Provide a brief summary of the Sanger sequencing m...\"\n",
      "[3] Skipped (already evaluated with valid metrics)\n",
      "[4] Processing question: \"Describe the structure and complementary base pair...\"\n",
      "[4] Skipped (already evaluated with valid metrics)\n",
      "[5] Processing question: \"How did the scientific community learn that DNA re...\"\n",
      "[5] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]Exception raised in Job[1]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499911, Requested 1030. Please try again in 2m42.527399999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  40%|████      | 2/5 [00:58<01:28, 29.39s/it]Exception raised in Job[3]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499639, Requested 1036. Please try again in 1m56.5642s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  60%|██████    | 3/5 [01:45<01:13, 36.73s/it]Exception raised in Job[2]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499519, Requested 1036. Please try again in 1m35.7672s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  80%|████████  | 4/5 [02:06<00:30, 30.78s/it]Exception raised in Job[0]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499480, Requested 1133. Please try again in 1m45.8378s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating: 100%|██████████| 5/5 [02:13<00:00, 26.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': nan, 'answer_relevancy': nan, 'factual_correctness(mode=f1)': nan, 'factual_correctness(mode=recall)': nan, 'semantic_similarity': 0.4424}\n",
      "[5] Evaluation completed successfully.\n",
      "[6] Processing question: \"DNA replication is bidirectional and discontinuous...\"\n",
      "[6] Skipped (already evaluated with valid metrics)\n",
      "[7] Processing question: \"What are Okazaki fragments and how they are formed...\"\n",
      "[7] Skipped (already evaluated with valid metrics)\n",
      "[8] Processing question: \"If the rate of replication in a particular prokary...\"\n",
      "[8] Skipped (already evaluated with valid metrics)\n",
      "[9] Processing question: \"Explain the events taking place at the replication...\"\n",
      "[9] Skipped (already evaluated with valid metrics)\n",
      "[10] Processing question: \"What is the role of a primer in DNA replication? W...\"\n",
      "[10] Skipped (already evaluated with valid metrics)\n",
      "[11] Processing question: \"How do the linear chromosomes in eukaryotes ensure...\"\n",
      "[11] Skipped (already evaluated with valid metrics)\n",
      "[12] Processing question: \"What is the consequence of mutation of a mismatch ...\"\n",
      "[12] Skipped (already evaluated with valid metrics)\n",
      "[13] Processing question: \"Imagine if there were 200 commonly occurring amino...\"\n",
      "[13] Skipped (already evaluated with valid metrics)\n",
      "[14] Processing question: \"Discuss how degeneracy of the genetic code makes c...\"\n",
      "[14] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]Exception raised in Job[1]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499292, Requested 1081. Please try again in 1m4.4532s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  40%|████      | 2/5 [00:26<00:39, 13.08s/it]Exception raised in Job[3]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500931, Requested 1088. Please try again in 5m49.0482s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  60%|██████    | 3/5 [01:42<01:18, 39.47s/it]Exception raised in Job[2]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500582, Requested 1088. Please try again in 4m48.6532s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  80%|████████  | 4/5 [02:42<00:47, 47.28s/it]Exception raised in Job[0]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500565, Requested 1179. Please try again in 5m1.394s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating: 100%|██████████| 5/5 [02:45<00:00, 33.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': nan, 'answer_relevancy': nan, 'factual_correctness(mode=f1)': nan, 'factual_correctness(mode=recall)': nan, 'semantic_similarity': 0.7078}\n",
      "[14] Evaluation completed successfully.\n",
      "[15] Processing question: \"If mRNA is complementary to the DNA template stran...\"\n",
      "[15] Skipped (already evaluated with valid metrics)\n",
      "[16] Processing question: \"In your own words, describe the difference between...\"\n",
      "[16] Skipped (already evaluated with valid metrics)\n",
      "[17] Processing question: \"Transcribe and translate the following DNA sequenc...\"\n",
      "[17] Skipped (already evaluated with valid metrics)\n",
      "[18] Processing question: \"Explain how single nucleotide changes can have vas...\"\n",
      "[18] Skipped (already evaluated with valid metrics)\n",
      "[19] Processing question: \"Name two differences between prokaryotic and eukar...\"\n",
      "[19] Skipped (already evaluated with valid metrics)\n",
      "[20] Processing question: \"Describe how controlling gene expression will alte...\"\n",
      "[20] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]Exception raised in Job[1]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499958, Requested 1058. Please try again in 2m55.4752s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  40%|████      | 2/5 [01:37<02:26, 48.71s/it]Exception raised in Job[0]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499773, Requested 1159. Please try again in 2m41.028999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  60%|██████    | 3/5 [02:09<01:23, 41.71s/it]Exception raised in Job[2]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499769, Requested 1065. Please try again in 2m23.9798s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  80%|████████  | 4/5 [02:10<00:26, 26.45s/it]Exception raised in Job[3]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499722, Requested 1065. Please try again in 2m15.932799999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating: 100%|██████████| 5/5 [02:18<00:00, 27.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': nan, 'answer_relevancy': nan, 'factual_correctness(mode=f1)': nan, 'factual_correctness(mode=recall)': nan, 'semantic_similarity': 0.7265}\n",
      "[20] Evaluation completed successfully.\n",
      "[21] Processing question: \"Describe how transcription in prokaryotic cells ca...\"\n",
      "[21] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]Exception raised in Job[0]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499391, Requested 1093. Please try again in 1m23.4652s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  40%|████      | 2/5 [00:51<01:17, 25.95s/it]Exception raised in Job[1]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500780, Requested 983. Please try again in 5m4.7328s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  60%|██████    | 3/5 [02:36<01:57, 58.83s/it]Exception raised in Job[2]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500725, Requested 990. Please try again in 4m56.3594s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  80%|████████  | 4/5 [02:46<00:40, 40.46s/it]Exception raised in Job[3]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 5/5 [03:00<00:00, 36.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': nan, 'answer_relevancy': nan, 'factual_correctness(mode=f1)': nan, 'factual_correctness(mode=recall)': nan, 'semantic_similarity': 0.7626}\n",
      "[21] Evaluation completed successfully.\n",
      "[22] Processing question: \"What is the difference between a repressible and a...\"\n",
      "[22] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]Exception raised in Job[1]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500097, Requested 919. Please try again in 2m55.671599999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  40%|████      | 2/5 [01:34<02:21, 47.16s/it]Exception raised in Job[2]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500044, Requested 926. Please try again in 2m47.6402s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  60%|██████    | 3/5 [01:43<01:02, 31.36s/it]Exception raised in Job[3]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499886, Requested 926. Please try again in 2m20.1972s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  80%|████████  | 4/5 [02:10<00:29, 29.90s/it]Exception raised in Job[0]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499843, Requested 1014. Please try again in 2m28.0326s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating: 100%|██████████| 5/5 [02:18<00:00, 27.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': nan, 'answer_relevancy': nan, 'factual_correctness(mode=f1)': nan, 'factual_correctness(mode=recall)': nan, 'semantic_similarity': 0.7918}\n",
      "[22] Evaluation completed successfully.\n",
      "[23] Processing question: \"In cancer cells, alteration to epigenetic modifica...\"\n",
      "[23] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]Exception raised in Job[2]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500853, Requested 844. Please try again in 4m53.4122s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  40%|████      | 2/5 [01:58<02:57, 59.22s/it]Exception raised in Job[3]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500615, Requested 844. Please try again in 4m12.1312s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  60%|██████    | 3/5 [02:39<01:43, 51.75s/it]Exception raised in Job[1]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500611, Requested 837. Please try again in 4m10.242599999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  80%|████████  | 4/5 [02:40<00:32, 32.69s/it]Exception raised in Job[0]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 5/5 [03:00<00:00, 36.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': nan, 'answer_relevancy': nan, 'factual_correctness(mode=f1)': nan, 'factual_correctness(mode=recall)': nan, 'semantic_similarity': 0.6882}\n",
      "[23] Evaluation completed successfully.\n",
      "[24] Processing question: \"A mutation within the promoter region can alter tr...\"\n",
      "[24] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]Exception raised in Job[2]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499929, Requested 826. Please try again in 2m10.353799999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  40%|████      | 2/5 [01:31<02:17, 45.73s/it]Exception raised in Job[3]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499879, Requested 826. Please try again in 2m1.8128s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  60%|██████    | 3/5 [01:40<01:00, 30.24s/it]Exception raised in Job[0]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499593, Requested 923. Please try again in 1m29.0294s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  80%|████████  | 4/5 [02:29<00:37, 37.44s/it]Exception raised in Job[1]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499570, Requested 819. Please try again in 1m7.098199999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating: 100%|██████████| 5/5 [02:33<00:00, 30.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': nan, 'answer_relevancy': nan, 'factual_correctness(mode=f1)': nan, 'factual_correctness(mode=recall)': nan, 'semantic_similarity': 0.8163}\n",
      "[24] Evaluation completed successfully.\n",
      "[25] Processing question: \"What could happen if a cell had too much of an act...\"\n",
      "[25] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  40%|████      | 2/5 [01:11<01:46, 35.66s/it]Exception raised in Job[0]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 503786, Requested 928. Please try again in 13m34.656s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  60%|██████    | 3/5 [02:46<02:00, 60.30s/it]Exception raised in Job[2]: TimeoutError()\n",
      "Exception raised in Job[3]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 5/5 [03:00<00:00, 36.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': nan, 'answer_relevancy': 0.9648, 'factual_correctness(mode=f1)': nan, 'factual_correctness(mode=recall)': nan, 'semantic_similarity': 0.7035}\n",
      "[25] Evaluation completed successfully.\n",
      "[26] Processing question: \"Protein modification can alter gene expression in ...\"\n",
      "[26] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]Exception raised in Job[1]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 503298, Requested 1058. Please try again in 12m32.734999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  40%|████      | 2/5 [01:06<01:39, 33.19s/it]Exception raised in Job[3]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 503026, Requested 1065. Please try again in 11m47.0226s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  60%|██████    | 3/5 [01:53<01:17, 38.91s/it]Exception raised in Job[2]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 502734, Requested 1065. Please try again in 10m56.6286s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  80%|████████  | 4/5 [02:43<00:43, 43.20s/it]Exception raised in Job[0]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 5/5 [03:00<00:00, 36.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': nan, 'answer_relevancy': nan, 'factual_correctness(mode=f1)': nan, 'factual_correctness(mode=recall)': nan, 'semantic_similarity': 0.6606}\n",
      "[26] Evaluation completed successfully.\n",
      "[27] Processing question: \"Alternative forms of a protein can be beneficial o...\"\n",
      "[27] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  20%|██        | 1/5 [00:00<00:00,  9.04it/s]Exception raised in Job[1]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 502221, Requested 788. Please try again in 8m40.118999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  40%|████      | 2/5 [01:08<02:00, 40.20s/it]Exception raised in Job[0]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 501911, Requested 915. Please try again in 8m8.4396s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  60%|██████    | 3/5 [02:02<01:32, 46.33s/it]Exception raised in Job[3]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 501807, Requested 795. Please try again in 7m29.6306s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  80%|████████  | 4/5 [02:20<00:35, 35.18s/it]Exception raised in Job[2]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 501617, Requested 795. Please try again in 6m56.9376s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating: 100%|██████████| 5/5 [02:52<00:00, 34.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': nan, 'answer_relevancy': nan, 'factual_correctness(mode=f1)': nan, 'factual_correctness(mode=recall)': nan, 'semantic_similarity': 0.6408}\n",
      "[27] Evaluation completed successfully.\n",
      "[28] Processing question: \"Changes in epigenetic modifications alter the acce...\"\n",
      "[28] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]Exception raised in Job[2]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 501192, Requested 1001. Please try again in 6m19.0334s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  40%|████      | 2/5 [01:05<01:38, 32.79s/it]Exception raised in Job[1]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500940, Requested 994. Please try again in 5m34.303799999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  60%|██████    | 3/5 [01:49<01:14, 37.26s/it]Exception raised in Job[0]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500917, Requested 1119. Please try again in 5m51.8898s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  80%|████████  | 4/5 [01:53<00:24, 24.85s/it]Exception raised in Job[3]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500719, Requested 1001. Please try again in 4m57.2464s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating: 100%|██████████| 5/5 [02:27<00:00, 29.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': nan, 'answer_relevancy': nan, 'factual_correctness(mode=f1)': nan, 'factual_correctness(mode=recall)': nan, 'semantic_similarity': 0.7476}\n",
      "[28] Evaluation completed successfully.\n",
      "[29] Processing question: \"New drugs are being developed that decrease DNA me...\"\n",
      "[29] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]Exception raised in Job[3]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500230, Requested 1549. Please try again in 5m7.419799999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  40%|████      | 2/5 [01:24<02:06, 42.05s/it]Exception raised in Job[1]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500153, Requested 1543. Please try again in 4m53.069s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  60%|██████    | 3/5 [01:37<01:00, 30.07s/it]Exception raised in Job[2]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499945, Requested 1549. Please try again in 4m18.0228s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  80%|████████  | 4/5 [02:13<00:32, 32.32s/it]Exception raised in Job[0]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499778, Requested 1671. Please try again in 4m10.2784s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating: 100%|██████████| 5/5 [02:42<00:00, 32.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': nan, 'answer_relevancy': nan, 'factual_correctness(mode=f1)': nan, 'factual_correctness(mode=recall)': nan, 'semantic_similarity': 0.5331}\n",
      "[29] Evaluation completed successfully.\n",
      "[30] Processing question: \"How can understanding the gene expression pattern ...\"\n",
      "[30] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  40%|████      | 2/5 [02:06<03:10, 63.48s/it]Exception raised in Job[0]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 503738, Requested 1037. Please try again in 13m45.2302s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  60%|██████    | 3/5 [02:34<01:36, 48.33s/it]Exception raised in Job[2]: TimeoutError()\n",
      "Exception raised in Job[3]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 5/5 [03:00<00:00, 36.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': nan, 'answer_relevancy': 0.9425, 'factual_correctness(mode=f1)': nan, 'factual_correctness(mode=recall)': nan, 'semantic_similarity': 0.7586}\n",
      "[30] Evaluation completed successfully.\n",
      "[31] Processing question: \"Describe the process of Southern blotting....\"\n",
      "[31] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  20%|██        | 1/5 [00:30<02:02, 30.62s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m test_set_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./test_data/genetics/test_set_method1_lightrag_hybrid.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results/genetics/method1_lightrag_hybrid.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mstart_evaluate_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_set_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m df_lr_hybrid2_genetics \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(output_file)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFILE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 52\u001b[0m, in \u001b[0;36mstart_evaluate_2\u001b[0;34m(test_set_file, output_file)\u001b[0m\n\u001b[1;32m     49\u001b[0m single_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict(single_data)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msingle_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43manswer_correctness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer_relevancy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFactualCorrectness\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mFactualCorrectness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSemanticSimilarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# results = evaluate(\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m#     dataset=single_dataset,\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m#     metrics=[answer_correctness, answer_relevancy, FactualCorrectness(atomicity=\"high\", coverage=\"high\"),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m#     embeddings=embedding\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mprint\u001b[39m(results)\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/ragas/_analytics.py:227\u001b[0m, in \u001b[0;36mtrack_was_completed.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m    226\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m--> 227\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/ragas/evaluation.py:294\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, experiment_name, callbacks, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar)\u001b[0m\n\u001b[1;32m    291\u001b[0m scores: t\u001b[38;5;241m.\u001b[39mList[t\u001b[38;5;241m.\u001b[39mDict[\u001b[38;5;28mstr\u001b[39m, t\u001b[38;5;241m.\u001b[39mAny]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;66;03m# get the results\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;241m==\u001b[39m []:\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/ragas/executor.py:213\u001b[0m, in \u001b[0;36mExecutor.results\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m             nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[1;32m    211\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nest_asyncio_applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_jobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m sorted_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(results, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [r[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m sorted_results]\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nest_asyncio.py:115\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m     heappop(scheduled)\n\u001b[1;32m    110\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    113\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 115\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[1;32m    118\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
      "File \u001b[0;32m/usr/lib/python3.10/selectors.py:469\u001b[0m, in \u001b[0;36mEpollSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    467\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 469\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[1]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 503124, Requested 995. Please try again in 11m51.8626s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[0]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 502956, Requested 1083. Please try again in 11m38.062s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[2]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 502900, Requested 1001. Please try again in 11m14.1374s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[3]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01jtgz94g3fdsbcwjdvkgqmjb3` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 502834, Requested 1001. Please try again in 11m2.7434s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/genetics/test_set_method1_lightrag_hybrid.json'\n",
    "output_file = './results/genetics/method1_lightrag_hybrid.csv'\n",
    "\n",
    "results = start_evaluate_2(test_set_file, output_file)\n",
    "\n",
    "df_lr_hybrid2_genetics = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_lr_hybrid2_genetics['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_lr_hybrid2_genetics['answer_relevancy'].mean())\n",
    "print(\"Average factual correctness(mode=f1):\", df_lr_hybrid2_genetics['factual_correctness(mode=f1)'].mean())\n",
    "print(\"Average factual correctness(mode=recall):\", df_lr_hybrid2_genetics['factual_correctness(mode=recall)'].mean())\n",
    "print(\"Average semantic similarity:\", df_lr_hybrid2_genetics['semantic_similarity'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3287c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_file = './test_data/genetics/test_set_method2_graphrag_drift.json'\n",
    "output_file = './results/genetics/method2_graphrag_drift.csv'\n",
    "\n",
    "#results = start_evaluate_2(test_set_file, output_file)\n",
    "\n",
    "df_gr_drift2_genetics = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_gr_drift2_genetics['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_gr_drift2_genetics['answer_relevancy'].mean())\n",
    "print(\"Average factual correctness(mode=f1):\", df_gr_drift2_genetics['factual_correctness(mode=f1)'].mean())\n",
    "print(\"Average factual correctness(mode=recall):\", df_gr_drift2_genetics['factual_correctness(mode=recall)'].mean())\n",
    "print(\"Average semantic similarity:\", df_gr_drift2_genetics['semantic_similarity'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1b8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_file = './test_data/genetics/test_set_naive_rag_5_top_k.json'\n",
    "output_file = './results/genetics/naive_rag_5_top_k.csv'\n",
    "\n",
    "#results = start_evaluate_2(test_set_file, output_file)\n",
    "\n",
    "df_naive_small_k2_genetics = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_naive_small_k2_genetics['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_naive_small_k2_genetics['answer_relevancy'].mean())\n",
    "print(\"Average factual correctness(mode=f1):\", df_naive_small_k2_genetics['factual_correctness(mode=f1)'].mean())\n",
    "print(\"Average factual correctness(mode=recall):\", df_naive_small_k2_genetics['factual_correctness(mode=recall)'].mean())\n",
    "print(\"Average semantic similarity:\", df_naive_small_k2_genetics['semantic_similarity'].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d84673aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macierz/s184306/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from ragas.metrics import answer_relevancy, answer_correctness, FactualCorrectness, SemanticSimilarity\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_localai import LocalAIEmbeddings\n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa962040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22976/1901439954.py:2: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_base=os.getenv(\"OPENAI_API_BASE\"), \n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),              \n",
    "    model_name=os.getenv(\"LLM_MODEL_NAME\")          \n",
    ")\n",
    "llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "embedding = LocalAIEmbeddings(\n",
    "    openai_api_base=os.getenv(\"EMBED_URL\"), \n",
    "    openai_api_key=os.getenv(\"EMBED_TOKEN\"), \n",
    "    model=os.getenv(\"EMBED_MODEL\")\n",
    ")\n",
    "embedding = LangchainEmbeddingsWrapper(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c59d835",
   "metadata": {},
   "source": [
    "# First Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537bcfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_evaluate(test_set_file, output_file):\n",
    "    with open(test_set_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        existing_df = pd.read_csv(output_file)\n",
    "    else:\n",
    "        existing_df = pd.DataFrame(columns=[\n",
    "            'user_input', 'response', 'reference',\n",
    "            'answer_correctness', 'answer_relevancy'\n",
    "        ])\n",
    "\n",
    "    existing_df.fillna('', inplace=True)\n",
    "\n",
    "    for idx, (q, a, g) in enumerate(zip(data['question'], data['answer'], data['ground_truth']), start=1):\n",
    "        short_q = q[:50].replace('\\n', ' ')\n",
    "        print(f\"[{idx}] Processing question: \\\"{short_q}...\\\"\")\n",
    "        if a == \"x\":\n",
    "            print(f\"[{idx}] Skipped (x in ans)\")\n",
    "            continue\n",
    "        match = existing_df[\n",
    "            (existing_df['user_input'] == q) &\n",
    "            (existing_df['response'] == a) &\n",
    "            (existing_df['reference'] == g)\n",
    "        ]\n",
    "        if not match.empty:\n",
    "            existing_row = match.iloc[0]\n",
    "            correctness = pd.to_numeric(existing_row['answer_correctness'], errors='coerce')\n",
    "            relevancy = pd.to_numeric(existing_row['answer_relevancy'], errors='coerce')\n",
    "            if (\n",
    "                not pd.isna(correctness) and not pd.isna(relevancy) and\n",
    "                correctness > 0.0 and relevancy > 0.0\n",
    "            ):\n",
    "                print(f\"[{idx}] Skipped (already evaluated with valid metrics)\")\n",
    "                continue\n",
    "\n",
    "        print(f\"[{idx}] Running evaluation...\")\n",
    "        single_data = {\n",
    "            \"question\": [q],\n",
    "            \"answer\": [a],\n",
    "            \"ground_truth\": [g]\n",
    "        }\n",
    "        single_dataset = Dataset.from_dict(single_data)\n",
    "\n",
    "        try:\n",
    "            results = evaluate(\n",
    "                dataset=single_dataset,\n",
    "                metrics=[answer_correctness, answer_relevancy],\n",
    "                llm=llm,\n",
    "                embeddings=embedding\n",
    "            )\n",
    "            row_df = results.to_pandas()\n",
    "            print(f\"[{idx}] Evaluation completed successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{idx}] Evaluation error: {e}\")\n",
    "            row_df = pd.DataFrame([{\n",
    "                'user_input': q,\n",
    "                'response': a,\n",
    "                'reference': g,\n",
    "                'answer_correctness': 0.0,\n",
    "                'answer_relevancy': 0.0\n",
    "            }])\n",
    "\n",
    "        existing_df = pd.concat([existing_df, row_df], ignore_index=True)\n",
    "        existing_df.drop_duplicates(\n",
    "            subset=['user_input', 'response', 'reference'], keep='last', inplace=True\n",
    "        )\n",
    "        existing_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"All done — results saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af39f85a",
   "metadata": {},
   "source": [
    "## Evaluation with meta-llama/llama-4-scout-17b-16e-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47ec78f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_base=os.getenv(\"OPENAI_API_BASE\"), \n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),              \n",
    "    model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\"         \n",
    ")\n",
    "llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "embedding = LocalAIEmbeddings(\n",
    "    openai_api_base=os.getenv(\"EMBED_URL\"), \n",
    "    openai_api_key=os.getenv(\"EMBED_TOKEN\"), \n",
    "    model=os.getenv(\"EMBED_MODEL\")\n",
    ")\n",
    "embedding = LangchainEmbeddingsWrapper(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4df1a4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/method1_lightrag_hybrid.csv\n",
      "Average answer correctness: 0.5122069836697539\n",
      "Average answer relevancy: 0.6982478951966491\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method1_lightrag_hybrid.json'\n",
    "output_file = './results/study_rules/method1_lightrag_hybrid.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_lr_hybrid = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_lr_hybrid['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_lr_hybrid['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3ef078b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/method1_lightrag_local.csv\n",
      "Average answer correctness: 0.4249733737262503\n",
      "Average answer relevancy: 0.6681967775090379\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method1_lightrag_local.json'\n",
    "output_file = './results/study_rules/method1_lightrag_local.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_lr_local = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_lr_local['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_lr_local['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6762cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/method2_graphrag_drift.csv\n",
      "Average answer correctness: 0.3500237400348909\n",
      "Average answer relevancy: 0.7774822101039724\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method2_graphrag_drift.json'\n",
    "output_file = './results/study_rules/method2_graphrag_drift.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_gr_drift = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_gr_drift['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_gr_drift['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bff0d871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/naive_rag.csv\n",
      "Average answer correctness: 0.5506225438324175\n",
      "Average answer relevancy: 0.7527904156645951\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_naive_rag.json'\n",
    "output_file = './results/study_rules/naive_rag.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_naive = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_naive['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_naive['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38a2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/naive_rag_small_top_k.csv\n",
      "Average answer correctness: 0.5652947016792146\n",
      "Average answer relevancy: 0.6714817258125219\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_naive_rag_small_top_k.json'\n",
    "output_file = './results/study_rules/naive_rag_small_top_k.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_naive_small_k = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_naive_small_k['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_naive_small_k['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0821265",
   "metadata": {},
   "source": [
    "## Evaluation with deepseek-r1-distill-llama-70b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d1518f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_base=os.getenv(\"OPENAI_API_BASE\"), \n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),              \n",
    "    model_name=\"deepseek-r1-distill-llama-70b\"         \n",
    ")\n",
    "llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "embedding = LocalAIEmbeddings(\n",
    "    openai_api_base=os.getenv(\"EMBED_URL\"), \n",
    "    openai_api_key=os.getenv(\"EMBED_TOKEN\"), \n",
    "    model=os.getenv(\"EMBED_MODEL\")\n",
    ")\n",
    "embedding = LangchainEmbeddingsWrapper(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "087bd949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/method1_lightrag_hybrid_deepseek.csv\n",
      "Average answer correctness: 0.49797992824280307\n",
      "Average answer relevancy: 0.7349126593819586\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method1_lightrag_hybrid.json'\n",
    "output_file = './results/study_rules/method1_lightrag_hybrid_deepseek.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_lr_hybrid_deepseek = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_lr_hybrid_deepseek['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_lr_hybrid_deepseek['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c04f9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/method1_lightrag_local_deepseek.csv\n",
      "Average answer correctness: 0.4016634365973334\n",
      "Average answer relevancy: 0.5804095482320186\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method1_lightrag_local.json'\n",
    "output_file = './results/study_rules/method1_lightrag_local_deepseek.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_lr_local_deepseek = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_lr_local_deepseek['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_lr_local_deepseek['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80a1c313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/method2_graphrag_drift_deepseek.csv\n",
      "Average answer correctness: 0.39543015771151035\n",
      "Average answer relevancy: 0.7339491973490562\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method2_graphrag_drift.json'\n",
    "output_file = './results/study_rules/method2_graphrag_drift_deepseek.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_gr_drift_deepseek = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_gr_drift_deepseek['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_gr_drift_deepseek['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6df536c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/naive_rag_deepseek.csv\n",
      "Average answer correctness: 0.5585839181481368\n",
      "Average answer relevancy: 0.7402449547731885\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_naive_rag.json'\n",
    "output_file = './results/study_rules/naive_rag_deepseek.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_naive_deepseek = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_naive_deepseek['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_naive_deepseek['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1811176d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/naive_rag_small_top_k_deepseek.csv\n",
      "Average answer correctness: 0.5452149662099343\n",
      "Average answer relevancy: 0.6653694964764589\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_naive_rag_small_top_k.json'\n",
    "output_file = './results/study_rules/naive_rag_small_top_k_deepseek.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_naive_small_k_deepseek = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_naive_small_k_deepseek['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_naive_small_k_deepseek['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deccca0",
   "metadata": {},
   "source": [
    "# Second Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fc59532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_evaluate_2(test_set_file, output_file):\n",
    "    with open(test_set_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        existing_df = pd.read_csv(output_file)\n",
    "    else:\n",
    "        existing_df = pd.DataFrame(columns=[\n",
    "            'user_input', 'response', 'reference',\n",
    "            'answer_correctness', 'answer_relevancy', 'factual_correctness(mode=f1)',\n",
    "            'factual_correctness(mode=recall)', 'semantic_similarity'\n",
    "        ])\n",
    "\n",
    "    existing_df.fillna('', inplace=True)\n",
    "\n",
    "    for idx, (q, a, g) in enumerate(zip(data['question'], data['answer'], data['ground_truth']), start=1):\n",
    "        short_q = q[:50].replace('\\n', ' ')\n",
    "        print(f\"[{idx}] Processing question: \\\"{short_q}...\\\"\")\n",
    "        if a == \"x\":\n",
    "            print(f\"[{idx}] Skipped (x in ans)\")\n",
    "            continue\n",
    "        match = existing_df[\n",
    "            (existing_df['user_input'] == q) &\n",
    "            (existing_df['response'] == a) &\n",
    "            (existing_df['reference'] == g)\n",
    "        ]\n",
    "        if not match.empty:\n",
    "            existing_row = match.iloc[0]\n",
    "            correctness = pd.to_numeric(existing_row['answer_correctness'], errors='coerce')\n",
    "            relevancy = pd.to_numeric(existing_row['answer_relevancy'], errors='coerce')\n",
    "            factual_correctness = pd.to_numeric(existing_row['factual_correctness(mode=f1)'], errors='coerce')\n",
    "            factual_correctness_recall = pd.to_numeric(existing_row['factual_correctness(mode=recall)'], errors='coerce')\n",
    "            semantic_similarity = pd.to_numeric(existing_row['semantic_similarity'], errors='coerce')\n",
    "            if (\n",
    "                not pd.isna(correctness) and not pd.isna(relevancy) and not pd.isna(factual_correctness) \n",
    "                and not pd.isna(factual_correctness_recall) and not pd.isna(semantic_similarity) and \n",
    "                correctness > 0.0 and relevancy > 0.0 and factual_correctness > 0.0  and factual_correctness_recall > 0.0\n",
    "                and semantic_similarity > 0.0\n",
    "            ):\n",
    "                print(f\"[{idx}] Skipped (already evaluated with valid metrics)\")\n",
    "                continue\n",
    "\n",
    "        print(f\"[{idx}] Running evaluation...\")\n",
    "        single_data = {\n",
    "            \"question\": [q],\n",
    "            \"answer\": [a],\n",
    "            \"ground_truth\": [g]\n",
    "        }\n",
    "        single_dataset = Dataset.from_dict(single_data)\n",
    "\n",
    "        try:\n",
    "            results = evaluate(\n",
    "                dataset=single_dataset,\n",
    "                metrics=[answer_correctness, answer_relevancy, FactualCorrectness(),\n",
    "                         FactualCorrectness(mode=\"recall\"), SemanticSimilarity()],\n",
    "                llm=llm,\n",
    "                embeddings=embedding\n",
    "            )\n",
    "            # results = evaluate(\n",
    "            #     dataset=single_dataset,\n",
    "            #     metrics=[answer_correctness, answer_relevancy, FactualCorrectness(atomicity=\"high\", coverage=\"high\"),\n",
    "            #              FactualCorrectness(mode=\"recall\",atomicity=\"high\", coverage=\"high\"), SemanticSimilarity()],\n",
    "            #     llm=llm,\n",
    "            #     embeddings=embedding\n",
    "            # )\n",
    "            print(results)\n",
    "            row_df = results.to_pandas()\n",
    "            print(f\"[{idx}] Evaluation completed successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{idx}] Evaluation error: {e}\")\n",
    "            row_df = pd.DataFrame([{\n",
    "                'user_input': q,\n",
    "                'response': a,\n",
    "                'reference': g,\n",
    "                'answer_correctness': 0.0,\n",
    "                'answer_relevancy': 0.0,\n",
    "                'factual_correctness(mode=f1)': 0.0,\n",
    "                'factual_correctness(mode=recall)': 0.0,\n",
    "                'semantic_similarity': 0.0\n",
    "            }])\n",
    "\n",
    "        existing_df = pd.concat([existing_df, row_df], ignore_index=True)\n",
    "        existing_df.drop_duplicates(\n",
    "            subset=['user_input', 'response', 'reference'], keep='last', inplace=True\n",
    "        )\n",
    "        existing_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"All done — results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5821634d",
   "metadata": {},
   "source": [
    "## Study Rules Evaluation with meta-llama/llama-4-scout-17b-16e-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6037f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_base=os.getenv(\"OPENAI_API_BASE\"), \n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),              \n",
    "    model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\"         \n",
    ")\n",
    "llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "embedding = LocalAIEmbeddings(\n",
    "    openai_api_base=os.getenv(\"EMBED_URL\"), \n",
    "    openai_api_key=os.getenv(\"EMBED_TOKEN\"), \n",
    "    model=os.getenv(\"EMBED_MODEL\")\n",
    ")\n",
    "embedding = LangchainEmbeddingsWrapper(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a1f993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules_2/method1_lightrag_hybrid.csv\n",
      "Average answer correctness: 0.5093708492159725\n",
      "Average answer relevancy: 0.6982478951966493\n",
      "Average factual correctness(mode=f1): 0.36714285714285716\n",
      "Average factual correctness(mode=recall): 0.3614285714285715\n",
      "Average semantic similarity: 0.7553770316263085\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method1_lightrag_hybrid.json'\n",
    "output_file = './results/study_rules_2/method1_lightrag_hybrid.csv'\n",
    "\n",
    "#results = start_evaluate_2(test_set_file, output_file)\n",
    "\n",
    "df_lr_hybrid2 = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_lr_hybrid2['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_lr_hybrid2['answer_relevancy'].mean())\n",
    "print(\"Average factual correctness(mode=f1):\", df_lr_hybrid2['factual_correctness(mode=f1)'].mean())\n",
    "print(\"Average factual correctness(mode=recall):\", df_lr_hybrid2['factual_correctness(mode=recall)'].mean())\n",
    "print(\"Average semantic similarity:\", df_lr_hybrid2['semantic_similarity'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b773ac51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules_2/method1_lightrag_local.csv\n",
      "Average answer correctness: 0.4249733737262503\n",
      "Average answer relevancy: 0.6681967775090379\n",
      "Average factual correctness(mode=f1): 0.2865\n",
      "Average factual correctness(mode=recall): 0.2575\n",
      "Average semantic similarity: 0.7379938908387216\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method1_lightrag_local.json'\n",
    "output_file = './results/study_rules_2/method1_lightrag_local.csv'\n",
    "\n",
    "#results = start_evaluate_2(test_set_file, output_file)\n",
    "\n",
    "df_lr_local2 = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_lr_local2['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_lr_local2['answer_relevancy'].mean())\n",
    "print(\"Average factual correctness(mode=f1):\", df_lr_local2['factual_correctness(mode=f1)'].mean())\n",
    "print(\"Average factual correctness(mode=recall):\", df_lr_local2['factual_correctness(mode=recall)'].mean())\n",
    "print(\"Average semantic similarity:\", df_lr_local2['semantic_similarity'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7c320bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules_2/method2_graphrag_drift.csv\n",
      "Average answer correctness: 0.3500237400348909\n",
      "Average answer relevancy: 0.7774822101039724\n",
      "Average factual correctness(mode=f1): 0.2935\n",
      "Average factual correctness(mode=recall): 0.34400000000000003\n",
      "Average semantic similarity: 0.7109516323324775\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method2_graphrag_drift.json'\n",
    "output_file = './results/study_rules_2/method2_graphrag_drift.csv'\n",
    "\n",
    "#results = start_evaluate_2(test_set_file, output_file)\n",
    "\n",
    "df_gr_drift2 = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_gr_drift2['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_gr_drift2['answer_relevancy'].mean())\n",
    "print(\"Average factual correctness(mode=f1):\", df_gr_drift2['factual_correctness(mode=f1)'].mean())\n",
    "print(\"Average factual correctness(mode=recall):\", df_gr_drift2['factual_correctness(mode=recall)'].mean())\n",
    "print(\"Average semantic similarity:\", df_gr_drift2['semantic_similarity'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cdd0561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules_2/naive_rag.csv\n",
      "Average answer correctness: 0.5502497388212984\n",
      "Average answer relevancy: 0.7530999940893139\n",
      "Average factual correctness(mode=f1): 0.4615\n",
      "Average factual correctness(mode=recall): 0.43050000000000005\n",
      "Average semantic similarity: 0.7995977826213055\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_naive_rag.json'\n",
    "output_file = './results/study_rules_2/naive_rag.csv'\n",
    "\n",
    "#results = start_evaluate_2(test_set_file, output_file)\n",
    "\n",
    "df_naive2 = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_naive2['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_naive2['answer_relevancy'].mean())\n",
    "print(\"Average factual correctness(mode=f1):\", df_naive2['factual_correctness(mode=f1)'].mean())\n",
    "print(\"Average factual correctness(mode=recall):\", df_naive2['factual_correctness(mode=recall)'].mean())\n",
    "print(\"Average semantic similarity:\", df_naive2['semantic_similarity'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8949f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules_2/naive_rag_small_top_k.csv\n",
      "Average answer correctness: 0.6082791145290342\n",
      "Average answer relevancy: 0.6331640165853277\n",
      "Average factual correctness(mode=f1): 0.5135714285714286\n",
      "Average factual correctness(mode=recall): 0.5121428571428571\n",
      "Average semantic similarity: 0.7969714609933498\n"
     ]
    }
   ],
   "source": [
    "#TODO run again\n",
    "test_set_file = './test_data/study_rules/test_set_naive_rag_small_top_k.json'\n",
    "output_file = './results/study_rules_2/naive_rag_small_top_k.csv'\n",
    "\n",
    "#results = start_evaluate_2(test_set_file, output_file)\n",
    "\n",
    "df_naive_small_k2 = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_naive_small_k2['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_naive_small_k2['answer_relevancy'].mean())\n",
    "print(\"Average factual correctness(mode=f1):\", df_naive_small_k2['factual_correctness(mode=f1)'].mean())\n",
    "print(\"Average factual correctness(mode=recall):\", df_naive_small_k2['factual_correctness(mode=recall)'].mean())\n",
    "print(\"Average semantic similarity:\", df_naive_small_k2['semantic_similarity'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd70f5c",
   "metadata": {},
   "source": [
    "## Genetics Evaluation with meta-llama/llama-4-scout-17b-16e-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa9055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d84673aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from ragas.metrics import answer_relevancy, answer_correctness\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_localai import LocalAIEmbeddings\n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b200923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/llama-4-scout-17b-16e-instruct\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "print(os.getenv(\"LLM_MODEL_NAME\"))\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_base=os.getenv(\"OPENAI_API_BASE\"), \n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),              \n",
    "    model_name=os.getenv(\"LLM_MODEL_NAME\")          \n",
    ")\n",
    "llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "embedding = LocalAIEmbeddings(\n",
    "    openai_api_base=os.getenv(\"EMBED_URL\"), \n",
    "    openai_api_key=os.getenv(\"EMBED_TOKEN\"), \n",
    "    model=os.getenv(\"EMBED_MODEL\")\n",
    ")\n",
    "embedding = LangchainEmbeddingsWrapper(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "537bcfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_evaluate(test_set_file, output_file):\n",
    "    with open(test_set_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        existing_df = pd.read_csv(output_file)\n",
    "    else:\n",
    "        existing_df = pd.DataFrame(columns=[\n",
    "            'user_input', 'response', 'reference',\n",
    "            'answer_correctness', 'answer_relevancy'\n",
    "        ])\n",
    "\n",
    "    existing_df.fillna('', inplace=True)\n",
    "\n",
    "    for idx, (q, a, g) in enumerate(zip(data['question'], data['answer'], data['ground_truth']), start=1):\n",
    "        short_q = q[:50].replace('\\n', ' ')\n",
    "        print(f\"[{idx}] Processing question: \\\"{short_q}...\\\"\")\n",
    "\n",
    "        match = existing_df[\n",
    "            (existing_df['user_input'] == q) &\n",
    "            (existing_df['response'] == a) &\n",
    "            (existing_df['reference'] == g)\n",
    "        ]\n",
    "        if not match.empty:\n",
    "            existing_row = match.iloc[0]\n",
    "            correctness = pd.to_numeric(existing_row['answer_correctness'], errors='coerce')\n",
    "            relevancy = pd.to_numeric(existing_row['answer_relevancy'], errors='coerce')\n",
    "            if (\n",
    "                not pd.isna(correctness) and not pd.isna(relevancy) and\n",
    "                correctness > 0 and relevancy > 0\n",
    "            ):\n",
    "                print(f\"[{idx}] Skipped (already evaluated with valid metrics)\")\n",
    "                continue\n",
    "\n",
    "        print(f\"[{idx}] Running evaluation...\")\n",
    "        single_data = {\n",
    "            \"question\": [q],\n",
    "            \"answer\": [a],\n",
    "            \"ground_truth\": [g]\n",
    "        }\n",
    "        single_dataset = Dataset.from_dict(single_data)\n",
    "\n",
    "        try:\n",
    "            results = evaluate(\n",
    "                dataset=single_dataset,\n",
    "                metrics=[answer_correctness, answer_relevancy],\n",
    "                llm=llm,\n",
    "                embeddings=embedding\n",
    "            )\n",
    "            row_df = results.to_pandas()\n",
    "            print(f\"[{idx}] Evaluation completed successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{idx}] Evaluation error: {e}\")\n",
    "            row_df = pd.DataFrame([{\n",
    "                'user_input': q,\n",
    "                'response': a,\n",
    "                'reference': g,\n",
    "                'answer_correctness': 0,\n",
    "                'answer_relevancy': 0\n",
    "            }])\n",
    "\n",
    "        existing_df = pd.concat([existing_df, row_df], ignore_index=True)\n",
    "        existing_df.drop_duplicates(\n",
    "            subset=['user_input', 'response', 'reference'], keep='last', inplace=True\n",
    "        )\n",
    "        existing_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"All done â€” results saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af39f85a",
   "metadata": {},
   "source": [
    "### LightRAG evaluation with meta-llama/llama-4-scout-17b-16e-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4df1a4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/method1_lightrag_hybrid.csv\n",
      "Average answer correctness: 0.5122069836697539\n",
      "Average answer relevancy: 0.6982478951966491\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method1_lightrag_hybrid.json'\n",
    "output_file = './results/study_rules/method1_lightrag_hybrid.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_lr_hybrid = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_lr_hybrid['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_lr_hybrid['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3ef078b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/method1_lightrag_local.csv\n",
      "Average answer correctness: 0.4249733737262503\n",
      "Average answer relevancy: 0.6681967775090379\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method1_lightrag_local.json'\n",
    "output_file = './results/study_rules/method1_lightrag_local.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_lr_local = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_lr_local['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_lr_local['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0821265",
   "metadata": {},
   "source": [
    "### LightRAG evaluation with deepseek-r1-distill-llama-70b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d1518f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_base=os.getenv(\"OPENAI_API_BASE\"), \n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),              \n",
    "    model_name=\"deepseek-r1-distill-llama-70b\"         \n",
    ")\n",
    "llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "embedding = LocalAIEmbeddings(\n",
    "    openai_api_base=os.getenv(\"EMBED_URL\"), \n",
    "    openai_api_key=os.getenv(\"EMBED_TOKEN\"), \n",
    "    model=os.getenv(\"EMBED_MODEL\")\n",
    ")\n",
    "embedding = LangchainEmbeddingsWrapper(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "087bd949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/method1_lightrag_hybrid_deepseek.csv\n",
      "Average answer correctness: 0.49797992824280307\n",
      "Average answer relevancy: 0.7349126593819586\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method1_lightrag_hybrid.json'\n",
    "output_file = './results/study_rules/method1_lightrag_hybrid_deepseek.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_lr_hybrid_deepseek = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_lr_hybrid_deepseek['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_lr_hybrid_deepseek['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c04f9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/method1_lightrag_local_deepseek.csv\n",
      "Average answer correctness: 0.4016634365973334\n",
      "Average answer relevancy: 0.5804095482320186\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method1_lightrag_local.json'\n",
    "output_file = './results/study_rules/method1_lightrag_local_deepseek.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_lr_local_deepseek = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_lr_local_deepseek['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_lr_local_deepseek['answer_relevancy'].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

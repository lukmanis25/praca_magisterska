{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d84673aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macierz/s184306/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from ragas.metrics import answer_relevancy, answer_correctness, FactualCorrectness, SemanticSimilarity\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_localai import LocalAIEmbeddings\n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa962040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_base=os.getenv(\"OPENAI_API_BASE\"), \n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),              \n",
    "    model_name=os.getenv(\"LLM_MODEL_NAME\")          \n",
    ")\n",
    "llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "embedding = LocalAIEmbeddings(\n",
    "    openai_api_base=os.getenv(\"EMBED_URL\"), \n",
    "    openai_api_key=os.getenv(\"EMBED_TOKEN\"), \n",
    "    model=os.getenv(\"EMBED_MODEL\")\n",
    ")\n",
    "embedding = LangchainEmbeddingsWrapper(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c59d835",
   "metadata": {},
   "source": [
    "# First Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537bcfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_evaluate(test_set_file, output_file):\n",
    "    with open(test_set_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        existing_df = pd.read_csv(output_file)\n",
    "    else:\n",
    "        existing_df = pd.DataFrame(columns=[\n",
    "            'user_input', 'response', 'reference',\n",
    "            'answer_correctness', 'answer_relevancy'\n",
    "        ])\n",
    "\n",
    "    existing_df.fillna('', inplace=True)\n",
    "\n",
    "    for idx, (q, a, g) in enumerate(zip(data['question'], data['answer'], data['ground_truth']), start=1):\n",
    "        short_q = q[:50].replace('\\n', ' ')\n",
    "        print(f\"[{idx}] Processing question: \\\"{short_q}...\\\"\")\n",
    "        if a == \"x\":\n",
    "            print(f\"[{idx}] Skipped (x in ans)\")\n",
    "            continue\n",
    "        match = existing_df[\n",
    "            (existing_df['user_input'] == q) &\n",
    "            (existing_df['response'] == a) &\n",
    "            (existing_df['reference'] == g)\n",
    "        ]\n",
    "        if not match.empty:\n",
    "            existing_row = match.iloc[0]\n",
    "            correctness = pd.to_numeric(existing_row['answer_correctness'], errors='coerce')\n",
    "            relevancy = pd.to_numeric(existing_row['answer_relevancy'], errors='coerce')\n",
    "            if (\n",
    "                not pd.isna(correctness) and not pd.isna(relevancy) and\n",
    "                correctness > 0.0 and relevancy > 0.0\n",
    "            ):\n",
    "                print(f\"[{idx}] Skipped (already evaluated with valid metrics)\")\n",
    "                continue\n",
    "\n",
    "        print(f\"[{idx}] Running evaluation...\")\n",
    "        single_data = {\n",
    "            \"question\": [q],\n",
    "            \"answer\": [a],\n",
    "            \"ground_truth\": [g]\n",
    "        }\n",
    "        single_dataset = Dataset.from_dict(single_data)\n",
    "\n",
    "        try:\n",
    "            results = evaluate(\n",
    "                dataset=single_dataset,\n",
    "                metrics=[answer_correctness, answer_relevancy],\n",
    "                llm=llm,\n",
    "                embeddings=embedding\n",
    "            )\n",
    "            row_df = results.to_pandas()\n",
    "            print(f\"[{idx}] Evaluation completed successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{idx}] Evaluation error: {e}\")\n",
    "            row_df = pd.DataFrame([{\n",
    "                'user_input': q,\n",
    "                'response': a,\n",
    "                'reference': g,\n",
    "                'answer_correctness': 0.0,\n",
    "                'answer_relevancy': 0.0\n",
    "            }])\n",
    "\n",
    "        existing_df = pd.concat([existing_df, row_df], ignore_index=True)\n",
    "        existing_df.drop_duplicates(\n",
    "            subset=['user_input', 'response', 'reference'], keep='last', inplace=True\n",
    "        )\n",
    "        existing_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"All done — results saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af39f85a",
   "metadata": {},
   "source": [
    "## Evaluation with meta-llama/llama-4-scout-17b-16e-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47ec78f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_base=os.getenv(\"OPENAI_API_BASE\"), \n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),              \n",
    "    model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\"         \n",
    ")\n",
    "llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "embedding = LocalAIEmbeddings(\n",
    "    openai_api_base=os.getenv(\"EMBED_URL\"), \n",
    "    openai_api_key=os.getenv(\"EMBED_TOKEN\"), \n",
    "    model=os.getenv(\"EMBED_MODEL\")\n",
    ")\n",
    "embedding = LangchainEmbeddingsWrapper(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4df1a4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/method1_lightrag_hybrid.csv\n",
      "Average answer correctness: 0.5122069836697539\n",
      "Average answer relevancy: 0.6982478951966491\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method1_lightrag_hybrid.json'\n",
    "output_file = './results/study_rules/method1_lightrag_hybrid.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_lr_hybrid = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_lr_hybrid['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_lr_hybrid['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3ef078b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/method1_lightrag_local.csv\n",
      "Average answer correctness: 0.4249733737262503\n",
      "Average answer relevancy: 0.6681967775090379\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method1_lightrag_local.json'\n",
    "output_file = './results/study_rules/method1_lightrag_local.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_lr_local = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_lr_local['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_lr_local['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6762cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/method2_graphrag_drift.csv\n",
      "Average answer correctness: 0.3500237400348909\n",
      "Average answer relevancy: 0.7774822101039724\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method2_graphrag_drift.json'\n",
    "output_file = './results/study_rules/method2_graphrag_drift.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_gr_drift = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_gr_drift['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_gr_drift['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bff0d871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/naive_rag.csv\n",
      "Average answer correctness: 0.5506225438324175\n",
      "Average answer relevancy: 0.7527904156645951\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_naive_rag.json'\n",
    "output_file = './results/study_rules/naive_rag.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_naive = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_naive['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_naive['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38a2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/naive_rag_small_top_k.csv\n",
      "Average answer correctness: 0.5652947016792146\n",
      "Average answer relevancy: 0.6714817258125219\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_naive_rag_small_top_k.json'\n",
    "output_file = './results/study_rules/naive_rag_small_top_k.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_naive_small_k = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_naive_small_k['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_naive_small_k['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0821265",
   "metadata": {},
   "source": [
    "## Evaluation with deepseek-r1-distill-llama-70b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d1518f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_base=os.getenv(\"OPENAI_API_BASE\"), \n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),              \n",
    "    model_name=\"deepseek-r1-distill-llama-70b\"         \n",
    ")\n",
    "llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "embedding = LocalAIEmbeddings(\n",
    "    openai_api_base=os.getenv(\"EMBED_URL\"), \n",
    "    openai_api_key=os.getenv(\"EMBED_TOKEN\"), \n",
    "    model=os.getenv(\"EMBED_MODEL\")\n",
    ")\n",
    "embedding = LangchainEmbeddingsWrapper(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "087bd949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/method1_lightrag_hybrid_deepseek.csv\n",
      "Average answer correctness: 0.49797992824280307\n",
      "Average answer relevancy: 0.7349126593819586\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method1_lightrag_hybrid.json'\n",
    "output_file = './results/study_rules/method1_lightrag_hybrid_deepseek.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_lr_hybrid_deepseek = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_lr_hybrid_deepseek['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_lr_hybrid_deepseek['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c04f9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/method1_lightrag_local_deepseek.csv\n",
      "Average answer correctness: 0.4016634365973334\n",
      "Average answer relevancy: 0.5804095482320186\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method1_lightrag_local.json'\n",
    "output_file = './results/study_rules/method1_lightrag_local_deepseek.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_lr_local_deepseek = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_lr_local_deepseek['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_lr_local_deepseek['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80a1c313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/method2_graphrag_drift_deepseek.csv\n",
      "Average answer correctness: 0.39543015771151035\n",
      "Average answer relevancy: 0.7339491973490562\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method2_graphrag_drift.json'\n",
    "output_file = './results/study_rules/method2_graphrag_drift_deepseek.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_gr_drift_deepseek = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_gr_drift_deepseek['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_gr_drift_deepseek['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6df536c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/naive_rag_deepseek.csv\n",
      "Average answer correctness: 0.5585839181481368\n",
      "Average answer relevancy: 0.7402449547731885\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_naive_rag.json'\n",
    "output_file = './results/study_rules/naive_rag_deepseek.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_naive_deepseek = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_naive_deepseek['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_naive_deepseek['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1811176d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules/naive_rag_small_top_k_deepseek.csv\n",
      "Average answer correctness: 0.5452149662099343\n",
      "Average answer relevancy: 0.6653694964764589\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_naive_rag_small_top_k.json'\n",
    "output_file = './results/study_rules/naive_rag_small_top_k_deepseek.csv'\n",
    "\n",
    "#results = start_evaluate(test_set_file, output_file)\n",
    "\n",
    "df_naive_small_k_deepseek = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_naive_small_k_deepseek['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_naive_small_k_deepseek['answer_relevancy'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deccca0",
   "metadata": {},
   "source": [
    "# Second Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fc59532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_evaluate_2(test_set_file, output_file):\n",
    "    with open(test_set_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        existing_df = pd.read_csv(output_file)\n",
    "    else:\n",
    "        existing_df = pd.DataFrame(columns=[\n",
    "            'user_input', 'response', 'reference',\n",
    "            'answer_correctness', 'answer_relevancy', 'factual_correctness(mode=f1)',\n",
    "            'factual_correctness(mode=recall)', 'semantic_similarity'\n",
    "        ])\n",
    "\n",
    "    existing_df.fillna('', inplace=True)\n",
    "\n",
    "    for idx, (q, a, g) in enumerate(zip(data['question'], data['answer'], data['ground_truth']), start=1):\n",
    "        short_q = q[:50].replace('\\n', ' ')\n",
    "        print(f\"[{idx}] Processing question: \\\"{short_q}...\\\"\")\n",
    "        if a == \"x\":\n",
    "            print(f\"[{idx}] Skipped (x in ans)\")\n",
    "            continue\n",
    "        match = existing_df[\n",
    "            (existing_df['user_input'] == q) &\n",
    "            (existing_df['response'] == a) &\n",
    "            (existing_df['reference'] == g)\n",
    "        ]\n",
    "        if not match.empty:\n",
    "            existing_row = match.iloc[0]\n",
    "            correctness = pd.to_numeric(existing_row['answer_correctness'], errors='coerce')\n",
    "            relevancy = pd.to_numeric(existing_row['answer_relevancy'], errors='coerce')\n",
    "            factual_correctness = pd.to_numeric(existing_row['factual_correctness(mode=f1)'], errors='coerce')\n",
    "            factual_correctness_recall = pd.to_numeric(existing_row['factual_correctness(mode=recall)'], errors='coerce')\n",
    "            semantic_similarity = pd.to_numeric(existing_row['semantic_similarity'], errors='coerce')\n",
    "            if (\n",
    "                not pd.isna(correctness) and not pd.isna(relevancy) and not pd.isna(factual_correctness) \n",
    "                and not pd.isna(factual_correctness_recall) and not pd.isna(semantic_similarity) and \n",
    "                correctness > 0.0 and relevancy > 0.0 and factual_correctness > 0.0  and factual_correctness_recall > 0.0\n",
    "                and semantic_similarity > 0.0\n",
    "            ):\n",
    "                print(f\"[{idx}] Skipped (already evaluated with valid metrics)\")\n",
    "                continue\n",
    "\n",
    "        print(f\"[{idx}] Running evaluation...\")\n",
    "        single_data = {\n",
    "            \"question\": [q],\n",
    "            \"answer\": [a],\n",
    "            \"ground_truth\": [g]\n",
    "        }\n",
    "        single_dataset = Dataset.from_dict(single_data)\n",
    "\n",
    "        try:\n",
    "            results = evaluate(\n",
    "                dataset=single_dataset,\n",
    "                metrics=[answer_correctness, answer_relevancy, FactualCorrectness(),\n",
    "                         FactualCorrectness(mode=\"recall\"), SemanticSimilarity()],\n",
    "                llm=llm,\n",
    "                embeddings=embedding\n",
    "            )\n",
    "            # results = evaluate(\n",
    "            #     dataset=single_dataset,\n",
    "            #     metrics=[answer_correctness, answer_relevancy, FactualCorrectness(atomicity=\"high\", coverage=\"high\"),\n",
    "            #              FactualCorrectness(mode=\"recall\",atomicity=\"high\", coverage=\"high\"), SemanticSimilarity()],\n",
    "            #     llm=llm,\n",
    "            #     embeddings=embedding\n",
    "            # )\n",
    "            print(results)\n",
    "            row_df = results.to_pandas()\n",
    "            print(f\"[{idx}] Evaluation completed successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{idx}] Evaluation error: {e}\")\n",
    "            row_df = pd.DataFrame([{\n",
    "                'user_input': q,\n",
    "                'response': a,\n",
    "                'reference': g,\n",
    "                'answer_correctness': 0.0,\n",
    "                'answer_relevancy': 0.0,\n",
    "                'factual_correctness(mode=f1)': 0.0,\n",
    "                'factual_correctness(mode=recall)': 0.0,\n",
    "                'semantic_similarity': 0.0\n",
    "            }])\n",
    "\n",
    "        existing_df = pd.concat([existing_df, row_df], ignore_index=True)\n",
    "        existing_df.drop_duplicates(\n",
    "            subset=['user_input', 'response', 'reference'], keep='last', inplace=True\n",
    "        )\n",
    "        existing_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"All done — results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5821634d",
   "metadata": {},
   "source": [
    "## Study Rules Evaluation with meta-llama/llama-4-scout-17b-16e-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6037f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_base=os.getenv(\"OPENAI_API_BASE\"), \n",
    "    openai_api_key=\"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\", #os.getenv(\"OPENAI_API_KEY\"),              \n",
    "    model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\"         \n",
    ")\n",
    "llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "embedding = LocalAIEmbeddings(\n",
    "    openai_api_base=os.getenv(\"EMBED_URL\"), \n",
    "    openai_api_key=os.getenv(\"EMBED_TOKEN\"), \n",
    "    model=os.getenv(\"EMBED_MODEL\")\n",
    ")\n",
    "embedding = LangchainEmbeddingsWrapper(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a1f993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules_2/method1_lightrag_hybrid.csv\n",
      "Average answer correctness: 0.5093708492159725\n",
      "Average answer relevancy: 0.6982478951966493\n",
      "Average factual correctness(mode=f1): 0.36714285714285716\n",
      "Average factual correctness(mode=recall): 0.3614285714285715\n",
      "Average semantic similarity: 0.7553770316263085\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method1_lightrag_hybrid.json'\n",
    "output_file = './results/study_rules_2/method1_lightrag_hybrid.csv'\n",
    "\n",
    "#results = start_evaluate_2(test_set_file, output_file)\n",
    "\n",
    "df_lr_hybrid2 = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_lr_hybrid2['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_lr_hybrid2['answer_relevancy'].mean())\n",
    "print(\"Average factual correctness(mode=f1):\", df_lr_hybrid2['factual_correctness(mode=f1)'].mean())\n",
    "print(\"Average factual correctness(mode=recall):\", df_lr_hybrid2['factual_correctness(mode=recall)'].mean())\n",
    "print(\"Average semantic similarity:\", df_lr_hybrid2['semantic_similarity'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b773ac51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules_2/method1_lightrag_local.csv\n",
      "Average answer correctness: 0.4249733737262503\n",
      "Average answer relevancy: 0.6681967775090379\n",
      "Average factual correctness(mode=f1): 0.2865\n",
      "Average factual correctness(mode=recall): 0.2575\n",
      "Average semantic similarity: 0.7379938908387216\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method1_lightrag_local.json'\n",
    "output_file = './results/study_rules_2/method1_lightrag_local.csv'\n",
    "\n",
    "#results = start_evaluate_2(test_set_file, output_file)\n",
    "\n",
    "df_lr_local2 = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_lr_local2['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_lr_local2['answer_relevancy'].mean())\n",
    "print(\"Average factual correctness(mode=f1):\", df_lr_local2['factual_correctness(mode=f1)'].mean())\n",
    "print(\"Average factual correctness(mode=recall):\", df_lr_local2['factual_correctness(mode=recall)'].mean())\n",
    "print(\"Average semantic similarity:\", df_lr_local2['semantic_similarity'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7c320bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules_2/method2_graphrag_drift.csv\n",
      "Average answer correctness: 0.3500237400348909\n",
      "Average answer relevancy: 0.7774822101039724\n",
      "Average factual correctness(mode=f1): 0.2935\n",
      "Average factual correctness(mode=recall): 0.34400000000000003\n",
      "Average semantic similarity: 0.7109516323324775\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_method2_graphrag_drift.json'\n",
    "output_file = './results/study_rules_2/method2_graphrag_drift.csv'\n",
    "\n",
    "#results = start_evaluate_2(test_set_file, output_file)\n",
    "\n",
    "df_gr_drift2 = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_gr_drift2['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_gr_drift2['answer_relevancy'].mean())\n",
    "print(\"Average factual correctness(mode=f1):\", df_gr_drift2['factual_correctness(mode=f1)'].mean())\n",
    "print(\"Average factual correctness(mode=recall):\", df_gr_drift2['factual_correctness(mode=recall)'].mean())\n",
    "print(\"Average semantic similarity:\", df_gr_drift2['semantic_similarity'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cdd0561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules_2/naive_rag.csv\n",
      "Average answer correctness: 0.5502497388212984\n",
      "Average answer relevancy: 0.7530999940893139\n",
      "Average factual correctness(mode=f1): 0.4615\n",
      "Average factual correctness(mode=recall): 0.43050000000000005\n",
      "Average semantic similarity: 0.7995977826213055\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_naive_rag.json'\n",
    "output_file = './results/study_rules_2/naive_rag.csv'\n",
    "\n",
    "#results = start_evaluate_2(test_set_file, output_file)\n",
    "\n",
    "df_naive2 = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_naive2['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_naive2['answer_relevancy'].mean())\n",
    "print(\"Average factual correctness(mode=f1):\", df_naive2['factual_correctness(mode=f1)'].mean())\n",
    "print(\"Average factual correctness(mode=recall):\", df_naive2['factual_correctness(mode=recall)'].mean())\n",
    "print(\"Average semantic similarity:\", df_naive2['semantic_similarity'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec8949f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: ./results/study_rules_2/naive_rag_small_top_k.csv\n",
      "Average answer correctness: 0.5536140471861982\n",
      "Average answer relevancy: 0.6617379602286304\n",
      "Average factual correctness(mode=f1): 0.45050000000000007\n",
      "Average factual correctness(mode=recall): 0.4425\n",
      "Average semantic similarity: 0.7969714609933498\n"
     ]
    }
   ],
   "source": [
    "test_set_file = './test_data/study_rules/test_set_naive_rag_small_top_k.json'\n",
    "output_file = './results/study_rules_2/naive_rag_small_top_k.csv'\n",
    "\n",
    "#results = start_evaluate_2(test_set_file, output_file)\n",
    "\n",
    "df_naive_small_k2 = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_naive_small_k2['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_naive_small_k2['answer_relevancy'].mean())\n",
    "print(\"Average factual correctness(mode=f1):\", df_naive_small_k2['factual_correctness(mode=f1)'].mean())\n",
    "print(\"Average factual correctness(mode=recall):\", df_naive_small_k2['factual_correctness(mode=recall)'].mean())\n",
    "print(\"Average semantic similarity:\", df_naive_small_k2['semantic_similarity'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd70f5c",
   "metadata": {},
   "source": [
    "## Genetics Evaluation with meta-llama/llama-4-scout-17b-16e-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa9055",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55732/4194908582.py:14: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  existing_df.fillna('', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Processing question: \"Explain Griffith's transformation experiments. Wha...\"\n",
      "[1] Skipped (already evaluated with valid metrics)\n",
      "[2] Processing question: \"Why were radioactive sulfur and phosphorous used t...\"\n",
      "[2] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:08<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': 0.3775, 'answer_relevancy': 0.7377, 'factual_correctness(mode=f1)': 0.0000, 'factual_correctness(mode=recall)': 0.0000, 'semantic_similarity': 0.5869}\n",
      "[2] Evaluation completed successfully.\n",
      "[3] Processing question: \"Provide a brief summary of the Sanger sequencing m...\"\n",
      "[3] Skipped (already evaluated with valid metrics)\n",
      "[4] Processing question: \"Describe the structure and complementary base pair...\"\n",
      "[4] Skipped (already evaluated with valid metrics)\n",
      "[5] Processing question: \"How did the scientific community learn that DNA re...\"\n",
      "[5] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:04<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': 0.4106, 'answer_relevancy': 0.8513, 'factual_correctness(mode=f1)': 0.0000, 'factual_correctness(mode=recall)': 0.0000, 'semantic_similarity': 0.4424}\n",
      "[5] Evaluation completed successfully.\n",
      "[6] Processing question: \"DNA replication is bidirectional and discontinuous...\"\n",
      "[6] Skipped (already evaluated with valid metrics)\n",
      "[7] Processing question: \"What are Okazaki fragments and how they are formed...\"\n",
      "[7] Skipped (already evaluated with valid metrics)\n",
      "[8] Processing question: \"If the rate of replication in a particular prokary...\"\n",
      "[8] Skipped (already evaluated with valid metrics)\n",
      "[9] Processing question: \"Explain the events taking place at the replication...\"\n",
      "[9] Skipped (already evaluated with valid metrics)\n",
      "[10] Processing question: \"What is the role of a primer in DNA replication? W...\"\n",
      "[10] Skipped (already evaluated with valid metrics)\n",
      "[11] Processing question: \"How do the linear chromosomes in eukaryotes ensure...\"\n",
      "[11] Skipped (already evaluated with valid metrics)\n",
      "[12] Processing question: \"What is the consequence of mutation of a mismatch ...\"\n",
      "[12] Skipped (already evaluated with valid metrics)\n",
      "[13] Processing question: \"Imagine if there were 200 commonly occurring amino...\"\n",
      "[13] Skipped (already evaluated with valid metrics)\n",
      "[14] Processing question: \"Discuss how degeneracy of the genetic code makes c...\"\n",
      "[14] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  80%|████████  | 4/5 [00:29<00:08,  8.33s/it]Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt correctness_classifier failed to parse output: The output parser failed to parse the output including retries.\n",
      "Exception raised in Job[0]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "Evaluating: 100%|██████████| 5/5 [01:09<00:00, 13.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': nan, 'answer_relevancy': 0.7778, 'factual_correctness(mode=f1)': 0.4700, 'factual_correctness(mode=recall)': 0.5700, 'semantic_similarity': 0.7078}\n",
      "[14] Evaluation completed successfully.\n",
      "[15] Processing question: \"If mRNA is complementary to the DNA template stran...\"\n",
      "[15] Skipped (already evaluated with valid metrics)\n",
      "[16] Processing question: \"In your own words, describe the difference between...\"\n",
      "[16] Skipped (already evaluated with valid metrics)\n",
      "[17] Processing question: \"Transcribe and translate the following DNA sequenc...\"\n",
      "[17] Skipped (already evaluated with valid metrics)\n",
      "[18] Processing question: \"Explain how single nucleotide changes can have vas...\"\n",
      "[18] Skipped (already evaluated with valid metrics)\n",
      "[19] Processing question: \"Name two differences between prokaryotic and eukar...\"\n",
      "[19] Skipped (already evaluated with valid metrics)\n",
      "[20] Processing question: \"Describe how controlling gene expression will alte...\"\n",
      "[20] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:28<00:00,  5.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': 0.6261, 'answer_relevancy': 0.6713, 'factual_correctness(mode=f1)': 0.5300, 'factual_correctness(mode=recall)': 0.4500, 'semantic_similarity': 0.7265}\n",
      "[20] Evaluation completed successfully.\n",
      "[21] Processing question: \"Describe how transcription in prokaryotic cells ca...\"\n",
      "[21] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:36<00:00,  7.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': 0.5121, 'answer_relevancy': 0.7432, 'factual_correctness(mode=f1)': 0.5500, 'factual_correctness(mode=recall)': 0.8900, 'semantic_similarity': 0.7626}\n",
      "[21] Evaluation completed successfully.\n",
      "[22] Processing question: \"What is the difference between a repressible and a...\"\n",
      "[22] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:42<00:00,  8.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': 0.4979, 'answer_relevancy': 0.4931, 'factual_correctness(mode=f1)': 0.2100, 'factual_correctness(mode=recall)': 0.5000, 'semantic_similarity': 0.7918}\n",
      "[22] Evaluation completed successfully.\n",
      "[23] Processing question: \"In cancer cells, alteration to epigenetic modifica...\"\n",
      "[23] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:28<00:00,  5.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': 0.4721, 'answer_relevancy': 0.9008, 'factual_correctness(mode=f1)': 0.0000, 'factual_correctness(mode=recall)': 1.0000, 'semantic_similarity': 0.6882}\n",
      "[23] Evaluation completed successfully.\n",
      "[24] Processing question: \"A mutation within the promoter region can alter tr...\"\n",
      "[24] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:31<00:00,  6.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': 0.5166, 'answer_relevancy': 0.8276, 'factual_correctness(mode=f1)': 0.6200, 'factual_correctness(mode=recall)': 0.8300, 'semantic_similarity': 0.8163}\n",
      "[24] Evaluation completed successfully.\n",
      "[25] Processing question: \"What could happen if a cell had too much of an act...\"\n",
      "[25] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:32<00:00,  6.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': 0.4066, 'answer_relevancy': 0.9672, 'factual_correctness(mode=f1)': 1.0000, 'factual_correctness(mode=recall)': 1.0000, 'semantic_similarity': 0.7035}\n",
      "[25] Evaluation completed successfully.\n",
      "[26] Processing question: \"Protein modification can alter gene expression in ...\"\n",
      "[26] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:37<00:00,  7.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': 0.4866, 'answer_relevancy': 0.7188, 'factual_correctness(mode=f1)': 0.4500, 'factual_correctness(mode=recall)': 0.6700, 'semantic_similarity': 0.6606}\n",
      "[26] Evaluation completed successfully.\n",
      "[27] Processing question: \"Alternative forms of a protein can be beneficial o...\"\n",
      "[27] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:37<00:00,  7.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': 0.4705, 'answer_relevancy': 0.7637, 'factual_correctness(mode=f1)': 0.0000, 'factual_correctness(mode=recall)': 0.0000, 'semantic_similarity': 0.6408}\n",
      "[27] Evaluation completed successfully.\n",
      "[28] Processing question: \"Changes in epigenetic modifications alter the acce...\"\n",
      "[28] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  60%|██████    | 3/5 [00:31<00:26, 13.23s/it]Exception raised in Job[0]: OutputParserException(Failed to parse StringIO from completion {\"TP\": [{\"statement\": \"Epigenetic modifications play a crucial role in regulating gene expression in response to environmental stimuli.\", \"reason\": \"The ground truth mentions that environmental stimuli can alter modifications to histone proteins or DNA, which implies a role in regulating gene expression.\"}, {\"statement\": \"Ultraviolet light exposure is an environmental stimulus.\", \"reason\": \"Directly supported by the ground truth which states that ultraviolet light exposure is an example of an environmental stimulus.\"}, {\"statement\": \"Epigenetic modifications can alter gene expression by changing the accessibility of DNA to transcription factors.\", \"reason\": \"Implied by the ground truth as it mentions changes in gene expression occur by modifying histone proteins or DNA.\"}, {\"statement\": \"Environmental stimuli can trigger epigenetic modifications that alter gene expression.\", \"reason\": \"Supported as the ground truth indicates environmental stimuli can change gene expression by altering histone or DNA modifications.\"}, {\"statement\": \"Epigenetic modifications can alter gene expression, leading to changes in cellular behavior.\", \"reason\": \"Implied by the ground truth though not directly stated, it aligns with the concept that changes in gene expression can have significant consequences.\"}], \"FP\": [{\"statement\": \"Ultraviolet light exposure can cause DNA damage.\", \"reason\": \"While plausible and related to UV exposure, the ground truth does not directly mention DNA damage from UV light.\"}, {\"statement\": \"DNA damage can lead to the formation of pyrimidine dimers.\", \"reason\": \"Not directly supported by the ground truth.\"}, {\"statement\": \"Pyrimidine dimers are a type of DNA lesion.\", \"reason\": \"Not directly supported by the ground truth.\"}, {\"statement\": \"Nucleotide excision repair and base excision repair are mechanisms for repairing DNA damage.\", \"reason\": \"Not mentioned in the ground truth.\"}, {\"statement\": \"Unrepaired pyrimidine dimers can lead to mutations and epigenetic changes.\", \"reason\": \"Not directly supported by the ground truth.\"}, {\"statement\": \"DNA methylation can silence gene expression by preventing the binding of transcription factors.\", \"reason\": \"While related, the ground truth does not specifically mention DNA methylation or its mechanism.\"}, {\"statement\": \"Histone acetylation can activate gene expression by relaxing chromatin structure.\", \"reason\": \"The ground truth mentions adding or removing groups from histones but not specifically histone acetylation's role.\"}, {\"statement\": \"The repair process can involve epigenetic modifications.\", \"reason\": \"Not directly mentioned in the ground truth.\"}, {\"statement\": \"Epigenetic modifications can be heritable.\", \"reason\": \"Not mentioned in the ground truth.\"}, {\"statement\": \"Ultraviolet light exposure can lead to skin cancer or other skin disorders.\", \"reason\": \"Not directly supported by the ground truth.\"}, {\"statement\": \"Changes in cellular behavior can have significant consequences for human health and disease.\", \"reason\": \"While implied, not directly stated in the ground truth.\"}, {\"statement\": \"Ultraviolet light exposure can lead to the formation of pyrimidine dimers.\", \"reason\": \"Not directly supported by the ground truth.\"}], \"FN\": [{\"statement\": \"Environmental stimuli can alter the modifications to histone proteins or DNA.\", \"reason\": \"Not directly mentioned in the answer though related concepts are covered.\"}, {\"statement\": \"The change in gene expression occurs by removing acetyl groups from histone proteins.\", \"reason\": \"Not mentioned in the answer.\"}, {\"statement\": \"The change in gene expression occurs by adding methyl groups to DNA.\", \"reason\": \"Not mentioned in the answer.\"}, {\"statement\": \"Environmental stimuli can change an actively transcribed gene into a silenced gene.\", \"reason\": \"Not directly addressed in the answer.\"}]}. Got: 1 validation error for StringIO\n",
      "text\n",
      "  Field required [type=missing, input_value={'TP': [{'statement': 'Ep...essed in the answer.'}]}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE )\n",
      "Evaluating: 100%|██████████| 5/5 [00:59<00:00, 11.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': nan, 'answer_relevancy': 0.9108, 'factual_correctness(mode=f1)': 0.4000, 'factual_correctness(mode=recall)': 0.6700, 'semantic_similarity': 0.7476}\n",
      "[28] Evaluation completed successfully.\n",
      "[29] Processing question: \"New drugs are being developed that decrease DNA me...\"\n",
      "[29] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:41<00:00,  8.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': 0.5083, 'answer_relevancy': 0.8439, 'factual_correctness(mode=f1)': 0.4200, 'factual_correctness(mode=recall)': 1.0000, 'semantic_similarity': 0.5331}\n",
      "[29] Evaluation completed successfully.\n",
      "[30] Processing question: \"How can understanding the gene expression pattern ...\"\n",
      "[30] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:34<00:00,  6.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': 0.5306, 'answer_relevancy': 0.9425, 'factual_correctness(mode=f1)': 0.7200, 'factual_correctness(mode=recall)': 0.8200, 'semantic_similarity': 0.7586}\n",
      "[30] Evaluation completed successfully.\n",
      "[31] Processing question: \"Describe the process of Southern blotting....\"\n",
      "[31] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:40<00:00,  8.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': 0.6728, 'answer_relevancy': 0.8856, 'factual_correctness(mode=f1)': 0.6300, 'factual_correctness(mode=recall)': 0.7500, 'semantic_similarity': 0.7966}\n",
      "[31] Evaluation completed successfully.\n",
      "[32] Processing question: \"A researcher wants to study cancer cells from a pa...\"\n",
      "[32] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:26<00:00,  5.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': 0.4702, 'answer_relevancy': 0.7414, 'factual_correctness(mode=f1)': 0.4000, 'factual_correctness(mode=recall)': 1.0000, 'semantic_similarity': 0.7378}\n",
      "[32] Evaluation completed successfully.\n",
      "[33] Processing question: \"How would a scientist introduce a gene for herbici...\"\n",
      "[33] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:33<00:00,  6.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': 0.5793, 'answer_relevancy': 0.9284, 'factual_correctness(mode=f1)': 0.5700, 'factual_correctness(mode=recall)': 1.0000, 'semantic_similarity': 0.8173}\n",
      "[33] Evaluation completed successfully.\n",
      "[34] Processing question: \"If you had a chance to get your genome sequenced, ...\"\n",
      "[34] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:28<00:00,  5.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': 0.3277, 'answer_relevancy': 0.5880, 'factual_correctness(mode=f1)': 0.5700, 'factual_correctness(mode=recall)': 0.6700, 'semantic_similarity': 0.3636}\n",
      "[34] Evaluation completed successfully.\n",
      "[35] Processing question: \"Why is so much effort being poured into genome map...\"\n",
      "[35] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:25<00:00,  5.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': 0.4726, 'answer_relevancy': 0.7201, 'factual_correctness(mode=f1)': 0.3800, 'factual_correctness(mode=recall)': 0.8000, 'semantic_similarity': 0.6904}\n",
      "[35] Evaluation completed successfully.\n",
      "[36] Processing question: \"How could a genetic map of the human genome help f...\"\n",
      "[36] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:29<00:00,  5.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': 0.4057, 'answer_relevancy': 0.8071, 'factual_correctness(mode=f1)': 0.3100, 'factual_correctness(mode=recall)': 0.5000, 'semantic_similarity': 0.7657}\n",
      "[36] Evaluation completed successfully.\n",
      "[37] Processing question: \"Explain why metagenomics is probably the most revo...\"\n",
      "[37] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:35<00:00,  7.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': 0.3973, 'answer_relevancy': 0.8571, 'factual_correctness(mode=f1)': 0.5000, 'factual_correctness(mode=recall)': 0.6700, 'semantic_similarity': 0.6802}\n",
      "[37] Evaluation completed successfully.\n",
      "[38] Processing question: \"How can genomics be used to predict disease risk a...\"\n",
      "[38] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:28<00:00,  5.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': 0.5202, 'answer_relevancy': 0.9390, 'factual_correctness(mode=f1)': 0.7500, 'factual_correctness(mode=recall)': 1.0000, 'semantic_similarity': 0.6690}\n",
      "[38] Evaluation completed successfully.\n",
      "[39] Processing question: \"How has proteomics been used in cancer detection a...\"\n",
      "[39] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:32<00:00,  6.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': 0.6084, 'answer_relevancy': 0.9403, 'factual_correctness(mode=f1)': 0.3000, 'factual_correctness(mode=recall)': 1.0000, 'semantic_similarity': 0.7973}\n",
      "[39] Evaluation completed successfully.\n",
      "[40] Processing question: \"What is personalized medicine?...\"\n",
      "[40] Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:31<00:00,  6.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_correctness': 0.3263, 'answer_relevancy': 0.9411, 'factual_correctness(mode=f1)': 0.3500, 'factual_correctness(mode=recall)': 0.8000, 'semantic_similarity': 0.7598}\n",
      "[40] Evaluation completed successfully.\n",
      "All done — results saved to: ./results/genetics/method1_lightrag_hybrid.csv\n",
      "FILE: ./results/genetics/method1_lightrag_hybrid.csv\n",
      "Average answer correctness: 0.4881001511656522\n",
      "Average answer relevancy: 0.8405683556795586\n",
      "Average factual correctness(mode=f1): 0.45675\n",
      "Average factual correctness(mode=recall): 0.725\n",
      "Average semantic similarity: 0.6850314441914681\n"
     ]
    }
   ],
   "source": [
    "#todo run again\n",
    "test_set_file = './test_data/genetics/test_set_method1_lightrag_hybrid.json'\n",
    "output_file = './results/genetics/method1_lightrag_hybrid.csv'\n",
    "\n",
    "#results = start_evaluate_2(test_set_file, output_file)\n",
    "\n",
    "df_lr_hybrid2_genetics = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_lr_hybrid2_genetics['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_lr_hybrid2_genetics['answer_relevancy'].mean())\n",
    "print(\"Average factual correctness(mode=f1):\", df_lr_hybrid2_genetics['factual_correctness(mode=f1)'].mean())\n",
    "print(\"Average factual correctness(mode=recall):\", df_lr_hybrid2_genetics['factual_correctness(mode=recall)'].mean())\n",
    "print(\"Average semantic similarity:\", df_lr_hybrid2_genetics['semantic_similarity'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3287c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_file = './test_data/genetics/test_set_method2_graphrag_drift.json'\n",
    "output_file = './results/genetics/method2_graphrag_drift.csv'\n",
    "\n",
    "#results = start_evaluate_2(test_set_file, output_file)\n",
    "\n",
    "df_gr_drift2_genetics = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_gr_drift2_genetics['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_gr_drift2_genetics['answer_relevancy'].mean())\n",
    "print(\"Average factual correctness(mode=f1):\", df_gr_drift2_genetics['factual_correctness(mode=f1)'].mean())\n",
    "print(\"Average factual correctness(mode=recall):\", df_gr_drift2_genetics['factual_correctness(mode=recall)'].mean())\n",
    "print(\"Average semantic similarity:\", df_gr_drift2_genetics['semantic_similarity'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1b8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_file = './test_data/genetics/test_set_naive_rag_5_top_k.json'\n",
    "output_file = './results/genetics/naive_rag_5_top_k.csv'\n",
    "\n",
    "#results = start_evaluate_2(test_set_file, output_file)\n",
    "\n",
    "df_naive_small_k2_genetics = pd.read_csv(output_file)\n",
    "print(f\"FILE: {output_file}\")\n",
    "print(\"Average answer correctness:\", df_naive_small_k2_genetics['answer_correctness'].mean())\n",
    "print(\"Average answer relevancy:\", df_naive_small_k2_genetics['answer_relevancy'].mean())\n",
    "print(\"Average factual correctness(mode=f1):\", df_naive_small_k2_genetics['factual_correctness(mode=f1)'].mean())\n",
    "print(\"Average factual correctness(mode=recall):\", df_naive_small_k2_genetics['factual_correctness(mode=recall)'].mean())\n",
    "print(\"Average semantic similarity:\", df_naive_small_k2_genetics['semantic_similarity'].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
